{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pgUWaBpdnzeT"
   },
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "\n",
    "spectrogran_size=[2,1,256,256]\n",
    "video_size=[2,int(30*5),1,16,256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8v2VoT7yl2X5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 29.958M\n",
      "Shape of out : torch.Size([1, 256, 256])\n",
      "dtype of out : torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "\n",
    "from transformers.models.vivit.modeling_vivit import VivitModel, VivitConfig, VivitLayer, VivitEncoder\n",
    "from transformers.models.speecht5.modeling_speecht5 import SpeechT5Decoder, SpeechT5Config\n",
    "\n",
    "class VivitTubeletEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct Vivit Tubelet embeddings.\n",
    "\n",
    "    This module turns a batch of videos of shape (batch_size, num_frames, num_channels, height, width) into a tensor of\n",
    "    shape (batch_size, seq_len, hidden_size) to be consumed by a Transformer encoder.\n",
    "\n",
    "    The seq_len (the number of patches) equals (number of frames // tubelet_size[0]) * (height // tubelet_size[1]) *\n",
    "    (width // tubelet_size[2]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_frames = config.num_frames\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.tubelet_size\n",
    "        self.num_patches = (\n",
    "            (self.image_size[1] // self.patch_size[2]) # 256/16\n",
    "            * (self.image_size[0] // self.patch_size[1]) # 16/4\n",
    "            * (self.num_frames // self.patch_size[0]) # 32/2 \n",
    "        )\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        self.projection = nn.Conv3d(\n",
    "            config.num_channels, config.hidden_size, kernel_size=config.tubelet_size, stride=config.tubelet_size\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
    "        if height != self.image_size[0] or width != self.image_size[1]:\n",
    "            raise ValueError(\n",
    "                f\"Input image size ({height},{width}) doesn't match model ({self.image_size},{self.image_size}).\"\n",
    "            )\n",
    "\n",
    "        # permute to (batch_size, num_channels, num_frames, height, width)\n",
    "        pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        x = self.projection(pixel_values)\n",
    "        # out_batch_size, out_num_channels, out_num_frames, out_height, out_width = x.shape\n",
    "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VivitEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Vivit Embeddings.\n",
    "\n",
    "    Creates embeddings from a video using VivitTubeletEmbeddings, adds CLS token and positional embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        self.patch_embeddings = VivitTubeletEmbeddings(config)\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, self.patch_embeddings.num_patches + 1, config.hidden_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "\n",
    "        cls_tokens = self.cls_token.tile([batch_size, 1, 1])\n",
    "\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class VivitPooler(nn.Module): #dont simply pool like this?\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output \n",
    "  \n",
    "class AiSynthModel(nn.Module):\n",
    "    def __init__(self, config, decoder, image_size=(16,256), tubelet_size=[2,4,16], num_frames = 32, dim = 192, num_layers=4, pool = 'mean', in_channels = 1, dim_head = 64, heads=4, dropout = 0.,\n",
    "                 emb_dropout = 0., scale_dim = 4, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        config.hidden_size = dim\n",
    "        config.num_channels = in_channels\n",
    "        config.num_frames = num_frames\n",
    "        config.tubelet_size = tubelet_size\n",
    "        config.image_size = image_size\n",
    "        config.num_attention_heads = heads\n",
    "        config.num_hidden_layers = num_layers\n",
    "        self.config = config\n",
    "\n",
    "        self.vivit_embeddings = VivitEmbeddings(config)\n",
    "\n",
    "        self.vivit_encoder = VivitEncoder(config)\n",
    "\n",
    "        self.speech_t5_decoder = decoder\n",
    "\n",
    "        \n",
    "        #nn.Sequential\n",
    "        #self.pooling = VivitPooler(config)\n",
    "\n",
    "        #define an operation to get from shape 1024x128 to \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vivit_embeddings(x)\n",
    "        #from second last dimension, drop the class tokens bcs we aren't doing classification\n",
    "        x = self.vivit_encoder(x).last_hidden_state[:,1:]\n",
    "        #x = self.pooling(x)\n",
    "        #downsample to size of spectrogram\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        #run through speech t5 decoder\n",
    "        x = self.speech_t5_decoder(x).last_hidden_state\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    img = torch.ones([1, 64, 1, 16, 256]).cuda()\n",
    "\n",
    "    config_vivit = VivitConfig()\n",
    "    conf_dict = {\n",
    "    \"activation_dropout\": 0.1,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"decoder_attention_heads\": 8,\n",
    "    \"decoder_ffn_dim\": 3072,\n",
    "    \"decoder_layerdrop\": 0.1,\n",
    "    \"decoder_layers\": 6,\n",
    "    \"decoder_start_token_id\": 2,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout\": 0.1,\n",
    "    \"hidden_size\": 256,\n",
    "    \"is_encoder_decoder\": True,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"mask_feature_length\": 4,\n",
    "    \"mask_feature_min_masks\": 0,\n",
    "    \"mask_feature_prob\": 0.0,\n",
    "    \"mask_time_length\": 4,\n",
    "    \"mask_time_min_masks\": 2,\n",
    "    \"positional_dropout\": 0.1,\n",
    "    \"transformers_version\": \"4.40.1\",\n",
    "    \"use_guided_attention_loss\": True,\n",
    "    }\n",
    "    config_speecht5 = SpeechT5Config(**conf_dict)\n",
    "\n",
    "    decoder = SpeechT5Decoder(config_speecht5)    \n",
    "    model = AiSynthModel(config_vivit, decoder, image_size=(16,256),tubelet_size=[2,8,32], num_frames = 64, dim = 512, num_layers=4).cuda()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "    print('Trainable Parameters: %.3fM' % parameters)\n",
    "    \n",
    "    out = model(img)\n",
    "    \n",
    "    print(\"Shape of out :\", out.shape)      # [B, num_classes]\n",
    "    print(\"dtype of out :\", out.dtype)      # float32\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "b0Vyssvil6HS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "#import wandb\n",
    "import sys,os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        # self.max_waveform_length = max_waveform_length\n",
    "        # self.max_num_frames = max_num_frames\n",
    "        self.frames = [file for file in os.listdir(os.path.join(root_dir, 'frames_pt')) if file.endswith('.pt')]\n",
    "        self.frames.sort()\n",
    "        self.graytransform = torchvision.transforms.Grayscale()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_file = os.path.join(self.root_dir, 'wavs', self.frames[idx].replace('.pt', '.wav'))\n",
    "        frames_path = os.path.join(self.root_dir, 'frames_pt', self.frames[idx])\n",
    "        spectrogram_path = os.path.join(self.root_dir, 'spectrograms_pt', self.frames[idx])\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(wav_file)\n",
    "\n",
    "        frames = torch.load(frames_path).unsqueeze(1).unsqueeze(0)\n",
    "        frames = self.graytransform(frames).float() / 127.5 - 1 # 0-1\n",
    "        spectrogram = torch.load(spectrogram_path).unsqueeze(0)\n",
    "\n",
    "        name = self.frames[idx].replace('.pt', '')\n",
    "        return frames, spectrogram, waveform, name\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, spectrogram, waveform, name = zip(*batch)\n",
    "    dat = {'frames':torch.vstack(frames).to(device=device), 'spectrogram':  torch.vstack(spectrogram).to(device=device), 'wav': torch.vstack(waveform).to(device=device), 'name':name}\n",
    "    return dat\n",
    "\n",
    "#init model\n",
    "batch_size=8\n",
    "train_dataset = CustomDataset(root_dir='data/processed/train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "val_dataset = CustomDataset(root_dir='data/processed/val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NQHH_cLU1Qj-"
   },
   "outputs": [],
   "source": [
    "from transformers import VivitConfig\n",
    "#model_enc = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vynVNV6iDlxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 23.646M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train: 100%|██████████| 22/22 [00:13<00:00,  1.68it/s, train_loss=44.9503, val_loss=175.4739]  \n",
      "Epoch 2/10 - Train: 100%|██████████| 22/22 [00:03<00:00,  6.05it/s, train_loss=44.9324, val_loss=175.4739] \n",
      "Epoch 3/10 - Train: 100%|██████████| 22/22 [00:11<00:00,  1.85it/s, train_loss=44.8930, val_loss=175.4096] \n",
      "Epoch 4/10 - Train: 100%|██████████| 22/22 [00:03<00:00,  6.14it/s, train_loss=44.8543, val_loss=175.4096] \n",
      "Epoch 5/10 - Train: 100%|██████████| 22/22 [00:12<00:00,  1.80it/s, train_loss=44.8699, val_loss=175.4067] \n",
      "Epoch 6/10 - Train: 100%|██████████| 22/22 [00:03<00:00,  6.16it/s, train_loss=44.8648, val_loss=175.4067] \n",
      "Epoch 7/10 - Train: 100%|██████████| 22/22 [00:12<00:00,  1.71it/s, train_loss=44.8516, val_loss=175.3966] \n",
      "Epoch 8/10 - Train: 100%|██████████| 22/22 [00:03<00:00,  6.04it/s, train_loss=44.8425, val_loss=175.3966] \n",
      "Epoch 9/10 - Train: 100%|██████████| 22/22 [00:03<00:00,  5.70it/s, train_loss=44.8778, val_loss=175.3966] \n",
      "Epoch 10/10 - Train:  14%|█▎        | 3/22 [00:09<00:57,  3.04s/it, train_loss=377.0463, val_loss=175.3966]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#wandb.log({\"val_loss\": val_loss.detach().cpu().item()}, step=step)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m resized_wavs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(val_out\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m122\u001b[39m,\u001b[38;5;241m122\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 94\u001b[0m out_wavs \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel_to_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_wavs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhann\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#use first channel of wav\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m#cross entropy loss for reconstructed wav and \"ground truth\" wav\u001b[39;00m\n\u001b[0;32m    107\u001b[0m val_loss_wav \u001b[38;5;241m=\u001b[39m criterion(torch\u001b[38;5;241m.\u001b[39mtensor(out_wavs), val_dat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\librosa\\feature\\inverse.py:192\u001b[0m, in \u001b[0;36mmel_to_audio\u001b[1;34m(M, sr, n_fft, hop_length, win_length, window, center, pad_mode, power, n_iter, length, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invert a mel power spectrogram to audio using Griffin-Lim.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03mThis is primarily a convenience wrapper for:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03mlibrosa.feature.inverse.mel_to_stft\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m stft \u001b[38;5;241m=\u001b[39m mel_to_stft(M, sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, power\u001b[38;5;241m=\u001b[39mpower, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgriffinlim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:2706\u001b[0m, in \u001b[0;36mgriffinlim\u001b[1;34m(S, n_iter, hop_length, win_length, n_fft, window, center, dtype, length, pad_mode, momentum, init, random_state)\u001b[0m\n\u001b[0;32m   2693\u001b[0m inverse \u001b[38;5;241m=\u001b[39m istft(\n\u001b[0;32m   2694\u001b[0m     angles,\n\u001b[0;32m   2695\u001b[0m     hop_length\u001b[38;5;241m=\u001b[39mhop_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2702\u001b[0m     out\u001b[38;5;241m=\u001b[39minverse,\n\u001b[0;32m   2703\u001b[0m )\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;66;03m# Rebuild the spectrogram\u001b[39;00m\n\u001b[1;32m-> 2706\u001b[0m rebuilt \u001b[38;5;241m=\u001b[39m \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrebuilt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2717\u001b[0m \u001b[38;5;66;03m# Update our phase estimates\u001b[39;00m\n\u001b[0;32m   2718\u001b[0m angles[:] \u001b[38;5;241m=\u001b[39m rebuilt\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:378\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[0;32m    376\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 378\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\numpy\\fft\\_pocketfft.py:118\u001b[0m, in \u001b[0;36m_fft_dispatcher\u001b[1;34m(a, n, axis, norm)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid norm value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    115\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mortho\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fft_dispatcher\u001b[39m(a, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_fft_dispatcher)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfft\u001b[39m(a, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_trace_dispatch_regular.py:326\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# ENDIF\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m''' This is the callback used when we enter some context in the debugger.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m        We also decorate the thread we are in with info about the debugging.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m            This is the global debugger (this method should actually be added as a method to it).\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;66;03m# IFDEF CYTHON\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# cdef str filename;\u001b[39;00m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# cdef str base;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;66;03m# DEBUG = 'code_to_debug' in frame.f_code.co_filename\u001b[39;00m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# if DEBUG: print('ENTER: trace_dispatch: %s %s %s %s' % (frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import librosa, wandb\n",
    "\n",
    "config_vivit = VivitConfig()\n",
    "conf_dict = {\n",
    "\"activation_dropout\": 0.1,\n",
    "\"attention_dropout\": 0.1,\n",
    "\"decoder_attention_heads\": 8,\n",
    "\"decoder_ffn_dim\": 3072,\n",
    "\"decoder_layerdrop\": 0.1,\n",
    "\"decoder_layers\": 3,\n",
    "\"decoder_start_token_id\": 2,\n",
    "\"hidden_act\": \"gelu\",\n",
    "\"hidden_dropout\": 0.1,\n",
    "\"hidden_size\": 256,\n",
    "\"is_encoder_decoder\": True,\n",
    "\"layer_norm_eps\": 1e-05,\n",
    "\"mask_feature_length\": 4,\n",
    "\"mask_feature_min_masks\": 0,\n",
    "\"mask_feature_prob\": 0.0,\n",
    "\"mask_time_length\": 4,\n",
    "\"mask_time_min_masks\": 2,\n",
    "\"positional_dropout\": 0.1,\n",
    "\"transformers_version\": \"4.40.1\",\n",
    "\"use_guided_attention_loss\": True,\n",
    "}\n",
    "config_speecht5 = SpeechT5Config(**conf_dict)\n",
    "\n",
    "decoder = SpeechT5Decoder(config_speecht5)    \n",
    "model = AiSynthModel(config_vivit, decoder, image_size=(16,256),tubelet_size=[2,8,32], num_frames = 64, dim = 512).cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "print('Trainable Parameters: %.3fM' % parameters)\n",
    "\n",
    "#opt\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "#loss fun\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"test_aisynth\",\n",
    "#     name=\"vid2audio\",\n",
    "#     job_type=\"training\",\n",
    "#     reinit=True)\n",
    "\n",
    "# %% Fit the model\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "step = 0\n",
    "val_interval=50\n",
    "#use tqdm to print train loss and val loss as updating instead of constantly printing\n",
    "\n",
    "\n",
    "tqdm \n",
    "\n",
    "val_loss = 10000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Train\") as pbar:\n",
    "\n",
    "        for dat in train_loader:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            out = model(dat['frames'])\n",
    "\n",
    "            loss = criterion(out, dat['spectrogram'])\n",
    "            #wandb.log({\"train_loss\": loss.detach().cpu().item()}, step=step)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({'train_loss': f'{loss:.4f}','val_loss': f'{val_loss:.4f}'})\n",
    "            pbar.update()\n",
    "            if step % val_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "\n",
    "                    val_dat = next(iter(val_loader))\n",
    "\n",
    "                    val_out = model(val_dat['frames'])\n",
    "\n",
    "                    val_loss = criterion(val_out, val_dat['spectrogram'])\n",
    "                    \n",
    "                    #wandb.log({\"val_loss\": val_loss.detach().cpu().item()}, step=step)\n",
    "                    resized_wavs = F.interpolate(val_out.unsqueeze(1), size=(122,122), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    out_wavs = librosa.feature.inverse.mel_to_audio(resized_wavs.cpu().squeeze().numpy(), \n",
    "                                            sr=16000, \n",
    "                                            n_fft=2048, \n",
    "                                            hop_length=512, \n",
    "                                            win_length=None, \n",
    "                                            window='hann', \n",
    "                                            center=False, \n",
    "                                            pad_mode='constant', \n",
    "                                            power=2.0, \n",
    "                                            n_iter=32)\n",
    "                    #use first channel of wav\n",
    "\n",
    "                    #cross entropy loss for reconstructed wav and \"ground truth\" wav\n",
    "                    val_loss_wav = criterion(torch.tensor(out_wavs), val_dat['wav'].detach().cpu())\n",
    "                    # wandb.log({\"val_loss_wav\": val_loss_wav.detach().cpu().item()}, step=step)\n",
    "\n",
    "\n",
    "            step += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "pCbTWo8vlKok",
    "outputId": "978442f8-2cc5-4597-ab99-75b131eb5867"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arifs\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% Fit the model\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for dat in tqdm(train_loader):\n",
    "\n",
    "\n",
    "        #wandb.log({\"loss\": loss.detach().cpu().item()}, step=step)\n",
    "        train_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpVPXUsxlpqT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfDsykaLlq5k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOHCPvEuGOfa"
   },
   "source": [
    "# Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 29.958M\n",
      "Shape of out : torch.Size([1, 256, 256])\n",
      "dtype of out : torch.float32\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5Decoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x SpeechT5DecoderLayer(\n",
       "      (self_attn): SpeechT5Attention(\n",
       "        (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): SpeechT5Attention(\n",
       "        (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): SpeechT5FeedForward(\n",
       "        (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (intermediate_dense): Linear(in_features=256, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "        (output_dense): Linear(in_features=3072, out_features=256, bias=True)\n",
       "        (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SpeechT5Decoder(config_speecht5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VivitConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"model_type\": \"vivit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 32,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.40.1\",\n",
       "  \"tubelet_size\": [\n",
       "    2,\n",
       "    16,\n",
       "    16\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
