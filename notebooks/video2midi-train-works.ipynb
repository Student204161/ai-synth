{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8208644,"sourceType":"datasetVersion","datasetId":4864249}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install av miditok py_midicsv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T14:33:46.528030Z","iopub.execute_input":"2024-05-17T14:33:46.528405Z","iopub.status.idle":"2024-05-17T14:34:02.414536Z","shell.execute_reply.started":"2024-05-17T14:33:46.528376Z","shell.execute_reply":"2024-05-17T14:34:02.413620Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting miditok\n  Downloading miditok-3.0.3-py3-none-any.whl.metadata (9.0 kB)\nCollecting py_midicsv\n  Downloading py_midicsv-4.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: huggingface-hub>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.22.2)\nRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from miditok) (1.26.4)\nCollecting symusic>=0.4.3 (from miditok)\n  Downloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.15.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from miditok) (4.66.1)\nRequirement already satisfied: rich-click<2.0.0,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from py_midicsv) (1.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (4.9.0)\nRequirement already satisfied: click>=7 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (8.1.7)\nRequirement already satisfied: rich>=10.7.0 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (13.7.0)\nCollecting pySmartDL (from symusic>=0.4.3->miditok)\n  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from symusic>=0.4.3->miditok) (4.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.16.4->miditok) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (2.17.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (2024.2.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (0.1.2)\nDownloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading miditok-3.0.3-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py_midicsv-4.0.0-py3-none-any.whl (16 kB)\nDownloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\nInstalling collected packages: pySmartDL, symusic, av, py_midicsv, miditok\nSuccessfully installed av-12.0.0 miditok-3.0.3 pySmartDL-1.3.4 py_midicsv-4.0.0 symusic-0.4.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import av\nimport numpy as np\nimport random\n\nfrom transformers import VivitImageProcessor, VivitModel, VivitConfig, TransfoXLLMHeadModel, TransfoXLConfig\nfrom huggingface_hub import hf_hub_download\n# print(transformers.__version__)\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport py_midicsv as pm\nimport torch\nimport torch.nn.functional as F\n\nfrom PIL import Image\nimport os\nfrom miditok import REMI, TokenizerConfig  # here we choose to use REMI\nimport miditok\nfrom pathlib import Path\nfrom torch.cuda.amp import autocast\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data.dataloader import default_collate\nfrom torch import optim\n\nfrom tqdm import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-17T14:34:02.416441Z","iopub.execute_input":"2024-05-17T14:34:02.416736Z","iopub.status.idle":"2024-05-17T14:34:19.711312Z","shell.execute_reply.started":"2024-05-17T14:34:02.416702Z","shell.execute_reply":"2024-05-17T14:34:19.710426Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-17 14:34:09.254294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 14:34:09.254394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 14:34:09.373566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# elle\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef cfg_to_dict(cfg):\n    return {attr: getattr(cfg, attr) for attr in dir(cfg) if not attr.startswith(\"__\") and not callable(getattr(cfg, attr))}\n\ndef read_frames_from_path(frames_path, indices, rgb=False):\n    '''\n    Read specific frames from a directory containing image files of video frames.\n    Args:\n        frames_path (str): Path to the directory containing frame images.\n        indices (List[int]): List of frame indices to read.\n    Returns:\n        result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n    '''\n    # List all files in the directory and sort them to maintain order\n    all_files = sorted(os.listdir(frames_path))\n    frames = []\n\n    # Process only files at specific indices\n    color_mode = 'RGB' if rgb else 'L'\n    for idx in indices:\n        if idx < len(all_files):\n            file_path = os.path.join(frames_path, all_files[idx])\n            with Image.open(file_path) as img:\n                # Convert image to RGB to ensure consistency\n                img = img.convert(color_mode)\n                # Calculate differences to make the image square\n                width, height = img.size\n                max_side = max(width, height)\n                # Create a new image with a black background\n                new_img = Image.new(color_mode, (max_side, max_side))\n                # Paste the original image onto the center of the new image\n                new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                frame_array = np.array(new_img)\n                if color_mode == 'L':\n                    # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                    frame_array = np.expand_dims(frame_array, axis=-1)\n                frames.append(frame_array)\n\n    stacked_frames = np.stack(frames, axis=0)\n    return stacked_frames\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices","metadata":{"execution":{"iopub.status.busy":"2024-05-17T14:34:19.713054Z","iopub.execute_input":"2024-05-17T14:34:19.713559Z","iopub.status.idle":"2024-05-17T14:34:19.730074Z","shell.execute_reply.started":"2024-05-17T14:34:19.713527Z","shell.execute_reply":"2024-05-17T14:34:19.728941Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Video2MIDIDataset(Dataset):\n    def __init__(self, root_dir, tokenizer, image_processor, transform=None, color_mode='gray'):\n        self.root_dir = root_dir\n        self.frames_dir = os.path.join(root_dir, 'frames')\n        self.midi_dir = os.path.join(root_dir, 'midi')\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.transform = transform\n        self.piece_names = [d for d in os.listdir(self.frames_dir) if os.path.isdir(os.path.join(self.frames_dir, d))]\n#         self.piece_names = self.piece_names[:2] # TODO: remove!!!\n        assert self.piece_names, f\"frame_dir at {self.frames_dir} is empty!\"\n        self.color_mode = 'L' if color_mode == 'gray' else 'RGB'\n\n    def __len__(self):\n        return len(self.piece_names)\n\n    def __getitem__(self, idx):\n        piece_name = self.piece_names[idx]\n        frames_path = os.path.join(self.frames_dir, piece_name)\n        midi_path = os.path.join(self.midi_dir, f'{piece_name}.mid')\n\n        midi_token_ids = self.load_midi(midi_path)\n        frames = self.load_frames(frames_path, rgb=True)\n#         img_side_width = 64\n#         img_size = (img_side_width,img_side_width)\n#         print(frames.shape, type(frames))\n        # NOTE (elle): MUST convert frames to list for some reason otherwise it complains!!!\n#         processed_frames = self.image_processor(list(frames), return_tensors=\"pt\", do_center_crop=False, do_resize=True, size=img_size)\n        processed_frames = image_processor(list(frames), return_tensors=\"pt\")\n#         print(processed_frames)\n\n        sample = {'frames': processed_frames['pixel_values'], 'midi_tokens': midi_token_ids}\n        return sample\n\n#     def load_frames(self, frames_path):\n#         frame_files = sorted(os.listdir(frames_path))\n#         frames = [Image.open(os.path.join(frames_path, f)).convert(self.color_mode) for f in frame_files]\n#         print(f\"img shape: {print(frames[0].size)}\")\n#         if self.transform:\n#             frames = [self.transform(frame) for frame in frames]\n#         return frames\n\n    def load_frames(self, frames_path, rgb=False):\n        '''\n        Read specific frames from a directory containing image files of video frames.\n        Args:\n            frames_path (str): Path to the directory containing frame images.\n            indices (List[int]): List of frame indices to read.\n        Returns:\n            result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n        '''\n        # List all files in the directory and sort them to maintain order\n        frame_names = sorted(os.listdir(frames_path))\n        frames = []\n\n        # Process only files at specific indices\n        color_mode = 'RGB' if rgb else 'L'\n        for frame_name in frame_names:\n            file_path = f\"{frames_path}/{frame_name}\"\n            with Image.open(file_path) as img:\n                # Convert image to RGB to ensure consistency\n                img = img.convert(color_mode)\n                # Calculate differences to make the image square\n                width, height = img.size\n                max_side = max(width, height)\n                # Create a new image with a black background\n                new_img = Image.new(color_mode, (max_side, max_side))\n                # Paste the original image onto the center of the new image\n                new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                frame_array = np.array(new_img)\n                if color_mode == 'L':\n                    # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                    frame_array = np.expand_dims(frame_array, axis=-1)\n                frames.append(frame_array)\n\n        stacked_frames = np.stack(frames, axis=0)\n        return stacked_frames\n\n    def load_midi(self, midi_path):\n        midi_tokens = self.tokenizer(midi_path)\n        midi_token_ids = torch.tensor(midi_tokens[0].ids, dtype=torch.long)\n        return midi_token_ids\n\ndef custom_collate_fn(batch, tokenizer):\n    # Extract frames and midi_tokens from the batch\n    frames = [item['frames'] for item in batch]\n    midi_tokens = [item['midi_tokens'] for item in batch]\n\n    # Pad the midi_tokens\n    # Assuming tokenizer provides PAD token index via tokenizer['PAD_None']\n    pad_token_index = tokenizer['PAD_None']  # Ensure this is the correct index for your PAD token\n    # print length before padding\n#     print(\"Length before padding: \", [len(midi_token) for midi_token in midi_tokens])\n    midi_tokens_padded = pad_sequence(midi_tokens, batch_first=True, padding_value=pad_token_index)\n#     print(\"Length after padding: \", [len(midi_token) for midi_token in midi_tokens_padded])\n\n    # Collate frames normally (assuming they are tensors of the same shape)\n    frames = default_collate(frames)\n\n    # Return a new dictionary with padded midi_tokens and frames\n    return {'frames': frames, 'midi_tokens': midi_tokens_padded}\n\ndef show_images_and_midi(dataloader):\n    for i, batch in enumerate(dataloader):\n        frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n        midi_tokens = batch['midi_tokens']  # MIDI tokens\n\n        print(f\"Batch {i + 1}\")\n\n        # Calculate the number of rows and columns for the subplots\n        batch_size = frames.size(0) # torch.Size([4, 1, 302, 1, 64, 64])\n#         n_igms_in_batch = frames.size(2)\n        cols = int(np.ceil(np.sqrt(batch_size)))\n        rows = int(np.ceil(batch_size / cols))\n\n        # Displaying images in a grid that's as square as possible\n        fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n        axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n        for j in range(batch_size):\n            # Select the first image from the video sequence and remove the singleton dimensions\n            img = frames[j, 0, 0, 0]  # Reduces to (64, 64)\n            axs[j].imshow(img.numpy())\n            axs[j].axis('off')  # Hide axes\n            axs[j].set_title(f'MIDI: {midi_tokens[j]}')  # Optionally print MIDI token IDs\n\n        # Hide any unused axes if the total number of subplots exceeds the batch size\n        for k in range(batch_size, len(axs)):\n            axs[k].axis('off')\n\n        plt.show()\n\n        # Optional: stop after first batch for demonstration\n        if i == 0:\n            break\n\ndef show_frames(frames):\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = len(frames)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[j]  # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.numpy())\n        axs[j].axis('off')  # Hide axes\n        axs[j].set_title(f'MIDI: {midi_tokens[0]}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    plt.show()\n\ndef show_images_and_midi_one_batch(dataloader, tokenizer):\n    # Fetch the first batch from the dataloader\n    token_id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n    batch = nextclas(iter(dataloader))\n    batch_i = 0\n    frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n    # ^ [4, 1, 32, 3, 224, 224]\n    midi_tokens = batch['midi_tokens']  # MIDI tokens\n    print(f\"miditokens shape {midi_tokens.shape}\")\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = frames.size(2)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    midi_translations = []\n    midis = []\n    tokenss = []\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[0, 0, j] # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.permute(1, 2, 0).numpy())\n        axs[j].axis('off')  # Hide axes\n        tokens = midi_tokens[batch_i]\n        tokenss.append(tokens)\n        midi = tokenizer([tokens])\n        midis.append(midi)\n        midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n        midi_translations.append(midi_translation)\n#         axs[j].set_title(f'MIDI: {midi_tokens_translated}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    title = f\"{midis[batch_i]}\\n{midi_translations[batch_i]}\\n{tokenss[batch_i]}\"\n    fig.suptitle(title, fontsize=16)\n    print(title)\n\n    plt.show()\n\n# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\ndef collate_fn(batch):\n    return custom_collate_fn(batch, tokenizer)\n\ndef compute_accuracy(outputs, labels):\n    logits = outputs.logits\n    prediction_ids = torch.argmax(logits, dim=-1)\n    # Flatten the tensors to compare each token\n    prediction_ids = prediction_ids.view(-1)\n    labels = labels.view(-1)\n    \n    # Compare predictions with labels\n    correct = (prediction_ids == labels).sum().item()\n    total = labels.size(0)\n\n    accuracy = correct / total\n    return accuracy, correct, total","metadata":{"execution":{"iopub.status.busy":"2024-05-17T14:36:04.832258Z","iopub.execute_input":"2024-05-17T14:36:04.832631Z","iopub.status.idle":"2024-05-17T14:36:04.867607Z","shell.execute_reply.started":"2024-05-17T14:36:04.832604Z","shell.execute_reply":"2024-05-17T14:36:04.866673Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# TODO\n* investigate why this training loop (using real data but just twinkle sequence 0, one second) seems to work now -- is it AdamW, the LR, or probably just that getting a transformer to overfit on random labels won't work because not properly padded with start/end token? although idk bc I feel like I tried it with non-random at first and it didn't work so truly I have no idea\n* why does the loss not quite go to 0 though? It hovers around .03 and then is a bit unstable and goes to .3 even sometimes e.g. a continuation of it although btw here https://wandb.ai/elles/video2music/runs/kmfa7au7?nw=nwuserellesummer I would think it should go to .0005ish which is what happened when I overfit to a single label sequence of the same numbers although at one point it did get to 0.0086 (not tracked in wandb). Just feel like it should be trivially easy/stable to memorize one one-sec sequence idk\n* BUT good news: despite loss not being ~0, the output of the model is still the given sequence, so all is good heh\n* ^^ take it further and track wer metric in general AH or just the accuracy tbh like per token (if it doesn't match for example)\n\nbtw for original model/dataset (not ours! aka the audio one)\nshapes:\nlabels: torch.Size([1, 67]). encoder_outputs: torch.Size([1, 112, 768]). labels: torch.Size([1, 22]). encoder_outputs: torch.Size([1, 113, 768]).\n\nour model tho is like: encoder_outputs: torch.Size([1, 3137, 768]) labels: torch.Size([1, 16])  \nwith 2 batch size  \nlabel shape torch.Size([2, 20])  \nframe shape torch.Size([2, 33, 3, 224, 224])  \nencoder_outputs shape torch.Size([2, 3137, 768])  ","metadata":{}},{"cell_type":"markdown","source":"## NEXT\n* last progress: able to fully overfit on twinkle 1 second clips but really slowly (converged to 100% accuracy after an hour yikes)\n* is slowness bc of the size of the encoder? is this worth the trade off of not having to train our own encoder? or is it bc the image quality has a lot of noise when resizing to fit the encoder needs? maybe worth using Khalil's modified encoder then...\n* will the model scale well to e.g. 2-5 second clips? (dimitrios suggested 5 sec) and across different songs?\n* try creating validation set just within twinkle twinkle? how to create a good one for this task in general? e.g. what should be \"allowed\" to overlap? We should see much less overlap in both inp/output when using longer sequences e.g. due to permutation diversity\n* debug why the original training didn't work for your own sanity/learning","metadata":{}},{"cell_type":"code","source":"from transformers import SpeechT5ForSpeechToText, SpeechT5Config\nfrom datasets import load_metric\n\nds_dir = \"../input/data-32fps/data_32fps\"\n# frames_path = f\"{ds_dir}/processed/frames/somebody--twinke_twinkle_0\"\n# frames_path = f\"{ds_dir}/processed/frames/raw--twinke_twinkle_0\"\n# Our parameters\nTOKENIZER_PARAMS = {\n    \"pitch_range\": (21, 109),\n    \"beat_res\": {(0, 4): 8, (4, 12): 4}, # TODO: finetune beat res?\n    \"num_velocities\": 32,\n    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n    \"use_chords\": True,\n    \"use_rests\": False,\n    \"use_tempos\": True,\n    \"use_time_signatures\": False,\n    \"use_programs\": False,\n    \"num_tempos\": 32,  # number of tempo bins\n    \"tempo_range\": (40, 250),  # (min, max)\n}\ntok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n# wer_metric = load_metric(\"wer\")\n# Creates the tokenizer\ntokenizer = REMI(tok_config)\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\ndataset = Video2MIDIDataset(\n    root_dir=f\"{ds_dir}/processed\",\n    tokenizer=tokenizer,\n    image_processor=image_processor\n    # transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n)\nprint(f\"Training on {len(dataset.piece_names)} pieces: {dataset.piece_names}\")\n\nconfig = SpeechT5Config(\n    vocab_size=328,\n    d_model=768,\n    max_length=450\n)\n\nmodel = SpeechT5ForSpeechToText(config)\nmodel = model.to('cuda')  # Ensure your model is on GPU if available\nmodel_enc = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\nmodel_enc = model_enc.to('cuda')\nmodel.speecht5.encoder = None\n\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 43\n    \n    LOG_EVERY_X_EPOCHS = 1\n\n    LR = 0.0001 #5e-5 # 4.786300923226385e-05 # 0.0001\n    EPOCHS = 100\n    BATCH_SIZE = 5\n\n    USE_WANDB = True\n    WANDB_PROJECT = \"video2music\"\n    WANDB_ENTITY = \"elles\"\n    WANDB_GROUP = f\"full_twinkle_bsz{BATCH_SIZE}_lr{LR}\"\n    EXPERIMENT = WANDB_GROUP\n    # FPS = 32\n\ncfg = Config()\n\nif cfg.SEED:\n    set_seed(cfg.SEED)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR)\n# Training loop\nmodel.train()\n\nif cfg.USE_WANDB:\n    # convert cfg class to dict\n    cfg_dict = cfg_to_dict(cfg)\n    cfg_dict.update(config.to_dict())\n    print(cfg_dict)\n    assert cfg_dict != {}, \"cfg_dict is empty\"\n    wandb_id = wandb.util.generate_id()\n    wandb.init(\n        project=cfg.WANDB_PROJECT,\n        name=cfg.EXPERIMENT,\n        entity=cfg.WANDB_ENTITY,\n        config=cfg_dict,\n        id=wandb_id,\n        resume=\"allow\",\n        group=cfg.WANDB_GROUP\n    )\n    wandb.watch(model, log=\"all\", log_freq=10)\n\nepochs = cfg.EPOCHS\n\nfor epoch in range(epochs):\n    total_correct = 0\n    total_tokens = 0\n    total_loss = 0\n    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}')\n    model.train()\n    for batch in progress_bar:\n  \n        optimizer.zero_grad()\n        frames = batch['frames'].to('cuda')\n        labels = batch['midi_tokens'].to('cuda')\n        with torch.no_grad():\n#             frames = batch['frames']\n\n            # labels = rand_labels.unsqueeze(0)\n#             labels = batch['midi_tokens']\n#             frames = frames.to('cuda')\n            # squeeze only if batch-size is > 1\n            frames = frames.squeeze(1)\n#             labels = labels.to('cuda')\n            outputs = model_enc(frames)\n            last_hidden_states = outputs.last_hidden_state\n#         new_encoder_outputs = last_hidden_states.to('cuda')\n        new_encoder_outputs = (last_hidden_states,)\n        inputs = {\n            \"encoder_outputs\": new_encoder_outputs,\n            \"labels\": labels # labels.unsqueeze(0) only unsqueeze if batch size is 1?\n        }\n        outputs = model(**inputs)\n\n        # Loss computation\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n        \n        progress_bar.set_postfix({'loss': loss.item()})\n\n    # Logging to wandb\n    average_loss = total_loss / len(dataloader)\n    if epoch % cfg.LOG_EVERY_X_EPOCHS == 0:\n        # TODO: add eval wer-metric\n        model.eval()\n        # TODO: iterate over training dataloader or eval dataloader\n        # because right now we're just using last batch output\n        with torch.no_grad():\n            inputs = {\n                \"encoder_outputs\": new_encoder_outputs,\n                \"labels\": labels #labels.unsqueeze(0) only unsqueeze if batch size is 1?\n            }\n            outputs = model(**inputs)\n            accuracy, batch_corr, batch_tot = compute_accuracy(outputs, labels)\n\n            # Accumulate results\n            # TODO fix: this is not accumulating per batch..\n            total_correct += batch_corr\n            total_tokens += batch_tot\n\n            overall_accuracy = total_correct / total_tokens\n            stats = {'epoch': epoch, 'epoch_avg_loss': average_loss, 'batch_accuracy': overall_accuracy}\n            print(stats)\n            if wandb.run:\n                wandb.log(stats)\n    del frames, labels, outputs, last_hidden_states, new_encoder_outputs, inputs\n    torch.cuda.empty_cache()\n\n# End of training\nif wandb.run:\n  wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T15:02:30.855855Z","iopub.execute_input":"2024-05-17T15:02:30.856283Z","iopub.status.idle":"2024-05-17T16:46:06.777195Z","shell.execute_reply.started":"2024-05-17T15:02:30.856253Z","shell.execute_reply":"2024-05-17T16:46:06.776451Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/872650869.py:21: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n  tok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n","output_type":"stream"},{"name":"stdout","text":"Training on 29 pieces: ['raw--twinke_twinkle_0', 'raw--twinke_twinkle_1', 'raw--twinke_twinkle_11', 'raw--twinke_twinkle_16', 'raw--twinke_twinkle_24', 'raw--twinke_twinkle_17', 'raw--twinke_twinkle_28', 'raw--twinke_twinkle_6', 'raw--twinke_twinkle_5', 'raw--twinke_twinkle_10', 'raw--twinke_twinkle_4', 'raw--twinke_twinkle_26', 'raw--twinke_twinkle_7', 'raw--twinke_twinkle_19', 'raw--twinke_twinkle_8', 'raw--twinke_twinkle_3', 'raw--twinke_twinkle_2', 'raw--twinke_twinkle_27', 'raw--twinke_twinkle_22', 'raw--twinke_twinkle_12', 'raw--twinke_twinkle_20', 'raw--twinke_twinkle_13', 'raw--twinke_twinkle_9', 'raw--twinke_twinkle_18', 'raw--twinke_twinkle_23', 'raw--twinke_twinkle_15', 'raw--twinke_twinkle_21', 'raw--twinke_twinkle_25', 'raw--twinke_twinkle_14']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'BATCH_SIZE': 5, 'DEVICE': 'cuda', 'EPOCHS': 100, 'EXPERIMENT': 'full_twinkle_bsz5_lr0.0001', 'LOG_EVERY_X_EPOCHS': 1, 'LR': 0.0001, 'SEED': 43, 'USE_WANDB': True, 'WANDB_ENTITY': 'elles', 'WANDB_GROUP': 'full_twinkle_bsz5_lr0.0001', 'WANDB_PROJECT': 'video2music', 'vocab_size': 328, 'hidden_size': 768, 'encoder_layers': 12, 'encoder_ffn_dim': 3072, 'encoder_attention_heads': 12, 'encoder_layerdrop': 0.1, 'decoder_layers': 6, 'decoder_ffn_dim': 3072, 'decoder_attention_heads': 12, 'decoder_layerdrop': 0.1, 'hidden_act': 'gelu', 'positional_dropout': 0.1, 'hidden_dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'scale_embedding': False, 'feat_extract_norm': 'group', 'feat_proj_dropout': 0.0, 'feat_extract_activation': 'gelu', 'conv_dim': [512, 512, 512, 512, 512, 512, 512], 'conv_stride': [5, 2, 2, 2, 2, 2, 2], 'conv_kernel': [10, 3, 3, 3, 3, 2, 2], 'conv_bias': False, 'num_conv_pos_embeddings': 128, 'num_conv_pos_embedding_groups': 16, 'num_feat_extract_layers': 7, 'apply_spec_augment': True, 'mask_time_prob': 0.05, 'mask_time_length': 10, 'mask_time_min_masks': 2, 'mask_feature_prob': 0.0, 'mask_feature_length': 10, 'mask_feature_min_masks': 0, 'num_mel_bins': 80, 'speech_decoder_prenet_layers': 2, 'speech_decoder_prenet_units': 256, 'speech_decoder_prenet_dropout': 0.5, 'speaker_embedding_dim': 512, 'speech_decoder_postnet_layers': 5, 'speech_decoder_postnet_units': 256, 'speech_decoder_postnet_kernel': 5, 'speech_decoder_postnet_dropout': 0.5, 'reduction_factor': 2, 'max_speech_positions': 4000, 'max_text_positions': 450, 'encoder_max_relative_position': 160, 'use_guided_attention_loss': True, 'guided_attention_loss_num_heads': 2, 'guided_attention_loss_sigma': 0.4, 'guided_attention_loss_scale': 10.0, 'use_cache': True, 'is_encoder_decoder': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 450, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': 2, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.39.3', 'd_model': 768, 'model_type': 'speecht5'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:wpxymnxb) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>▁▃▃▃▅▄▆▆▆▆▆▆▆▆█▆▆▆</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>epoch_avg_loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>0.5625</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>epoch_avg_loss</td><td>1.74242</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">full_twinkle_bsz5</strong> at: <a href='https://wandb.ai/elles/video2music/runs/wpxymnxb' target=\"_blank\">https://wandb.ai/elles/video2music/runs/wpxymnxb</a><br/> View project at: <a href='https://wandb.ai/elles/video2music' target=\"_blank\">https://wandb.ai/elles/video2music</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240517_143855-wpxymnxb/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:wpxymnxb). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_150234-58qujzdf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/elles/video2music/runs/58qujzdf' target=\"_blank\">full_twinkle_bsz5_lr0.0001</a></strong> to <a href='https://wandb.ai/elles/video2music' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/elles/video2music' target=\"_blank\">https://wandb.ai/elles/video2music</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/elles/video2music/runs/58qujzdf' target=\"_blank\">https://wandb.ai/elles/video2music/runs/58qujzdf</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=3.09]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 0, 'epoch_avg_loss': 3.875572935740153, 'batch_accuracy': 0.125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=3.1] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 1, 'epoch_avg_loss': 2.986229578653971, 'batch_accuracy': 0.3125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=2.5] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 2, 'epoch_avg_loss': 2.683447583516439, 'batch_accuracy': 0.375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=2.22]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 3, 'epoch_avg_loss': 2.486541016896566, 'batch_accuracy': 0.4375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=2.31]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 4, 'epoch_avg_loss': 2.483955224355062, 'batch_accuracy': 0.4375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=2]   \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 5, 'epoch_avg_loss': 2.3483480771382648, 'batch_accuracy': 0.4375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=2.04]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 6, 'epoch_avg_loss': 2.2885417222976683, 'batch_accuracy': 0.5}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=1.77]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 7, 'epoch_avg_loss': 2.234709922472636, 'batch_accuracy': 0.625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=1.92]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 8, 'epoch_avg_loss': 2.0932917833328246, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=1.74]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 9, 'epoch_avg_loss': 2.0936763286590576, 'batch_accuracy': 0.5}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=1.69]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 10, 'epoch_avg_loss': 1.978755251566569, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100: 100%|██████████| 15/15 [01:02<00:00,  4.13s/it, loss=1.76]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 11, 'epoch_avg_loss': 1.9408155679702759, 'batch_accuracy': 0.625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it, loss=1.46]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 12, 'epoch_avg_loss': 1.9219763199488322, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it, loss=1.56]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 13, 'epoch_avg_loss': 1.8605546553929646, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=1.33]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 14, 'epoch_avg_loss': 1.8282914241154988, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100: 100%|██████████| 15/15 [01:00<00:00,  4.07s/it, loss=1.2] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 15, 'epoch_avg_loss': 1.8297436714172364, 'batch_accuracy': 0.625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=1.7] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 16, 'epoch_avg_loss': 1.7950168132781983, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=1.21]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 17, 'epoch_avg_loss': 1.5653565168380736, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=0.895]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 18, 'epoch_avg_loss': 1.543097726504008, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=1.17]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 19, 'epoch_avg_loss': 1.6370347499847413, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it, loss=1.08]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 20, 'epoch_avg_loss': 1.4399619380633035, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=1.05]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 21, 'epoch_avg_loss': 1.3742711305618287, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=1.13]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 22, 'epoch_avg_loss': 1.398728370666504, 'batch_accuracy': 0.6875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it, loss=0.987]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 23, 'epoch_avg_loss': 1.5201578259468078, 'batch_accuracy': 0.625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=1.33]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 24, 'epoch_avg_loss': 1.4889368136723837, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it, loss=1.15]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 25, 'epoch_avg_loss': 1.3906919598579406, 'batch_accuracy': 0.875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=2.03]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 26, 'epoch_avg_loss': 1.3804047425587973, 'batch_accuracy': 0.5625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=0.972]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 27, 'epoch_avg_loss': 1.473443071047465, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=1]   \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 28, 'epoch_avg_loss': 1.4329034050305685, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it, loss=0.838]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 29, 'epoch_avg_loss': 1.3717004219690958, 'batch_accuracy': 0.5625}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it, loss=1.3] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 30, 'epoch_avg_loss': 1.332830560207367, 'batch_accuracy': 0.875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100: 100%|██████████| 15/15 [01:02<00:00,  4.16s/it, loss=1.18]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 31, 'epoch_avg_loss': 1.2985334992408752, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=0.948]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 32, 'epoch_avg_loss': 1.3632904966672261, 'batch_accuracy': 0.75}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=0.971]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 33, 'epoch_avg_loss': 1.271614098548889, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.889]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 34, 'epoch_avg_loss': 1.2520657857259114, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.913]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 35, 'epoch_avg_loss': 1.1572985410690309, 'batch_accuracy': 0.875}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=0.681]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 36, 'epoch_avg_loss': 1.2031017978986105, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=0.823]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 37, 'epoch_avg_loss': 1.2126583536465962, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=1.69] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 38, 'epoch_avg_loss': 1.210797949632009, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=0.711]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 39, 'epoch_avg_loss': 1.1257430791854859, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=0.745]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 40, 'epoch_avg_loss': 1.1504661957422893, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=0.615]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 41, 'epoch_avg_loss': 1.0824547131856284, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100: 100%|██████████| 15/15 [01:00<00:00,  4.04s/it, loss=0.724]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 42, 'epoch_avg_loss': 1.0975167473157248, 'batch_accuracy': 0.8125}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.638]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 43, 'epoch_avg_loss': 1.0904624938964844, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100: 100%|██████████| 15/15 [01:00<00:00,  4.07s/it, loss=0.638]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 44, 'epoch_avg_loss': 1.2233294169108073, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it, loss=0.645]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 45, 'epoch_avg_loss': 1.0305883169174195, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.547]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 46, 'epoch_avg_loss': 0.9892845630645752, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100: 100%|██████████| 15/15 [01:00<00:00,  4.07s/it, loss=0.64] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 47, 'epoch_avg_loss': 0.9993270953496297, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.707]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 48, 'epoch_avg_loss': 1.0691975553830464, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100: 100%|██████████| 15/15 [01:01<00:00,  4.07s/it, loss=0.552]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 49, 'epoch_avg_loss': 1.3050190091133118, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=0.854]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 50, 'epoch_avg_loss': 1.0600390791893006, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100: 100%|██████████| 15/15 [01:04<00:00,  4.27s/it, loss=0.535]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 51, 'epoch_avg_loss': 1.0248587528864543, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100: 100%|██████████| 15/15 [01:03<00:00,  4.23s/it, loss=0.452]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 52, 'epoch_avg_loss': 0.9534278452396393, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100: 100%|██████████| 15/15 [01:02<00:00,  4.18s/it, loss=0.429]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 53, 'epoch_avg_loss': 0.9119494954744974, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.332]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 54, 'epoch_avg_loss': 0.9493930379549662, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.413]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 55, 'epoch_avg_loss': 0.8756902098655701, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100: 100%|██████████| 15/15 [01:02<00:00,  4.19s/it, loss=0.303]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 56, 'epoch_avg_loss': 0.7631978074709574, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.334]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 57, 'epoch_avg_loss': 0.7815604587395986, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100: 100%|██████████| 15/15 [01:03<00:00,  4.23s/it, loss=0.736]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 58, 'epoch_avg_loss': 0.7455726524194082, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=0.172]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 59, 'epoch_avg_loss': 0.7581174671649933, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.29] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 60, 'epoch_avg_loss': 0.7799351056416829, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100: 100%|██████████| 15/15 [01:02<00:00,  4.18s/it, loss=0.192]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 61, 'epoch_avg_loss': 0.6730366537968318, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=0.179]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 62, 'epoch_avg_loss': 0.6538203845421473, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100: 100%|██████████| 15/15 [01:03<00:00,  4.21s/it, loss=0.148]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 63, 'epoch_avg_loss': 0.6906705856323242, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.286]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 64, 'epoch_avg_loss': 0.658273559808731, 'batch_accuracy': 0.9375}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it, loss=0.188]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 65, 'epoch_avg_loss': 0.6332839538653692, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it, loss=0.115]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 66, 'epoch_avg_loss': 0.5449103613694509, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.132]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 67, 'epoch_avg_loss': 0.7103353063265483, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.39] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 68, 'epoch_avg_loss': 0.8126632789770762, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=0.215]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 69, 'epoch_avg_loss': 0.6006777475277583, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it, loss=0.197]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 70, 'epoch_avg_loss': 0.5985211392243703, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100: 100%|██████████| 15/15 [01:02<00:00,  4.15s/it, loss=0.0809]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 71, 'epoch_avg_loss': 0.5197072053949038, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.218]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 72, 'epoch_avg_loss': 0.6735211700201035, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100: 100%|██████████| 15/15 [01:02<00:00,  4.16s/it, loss=0.0454]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 73, 'epoch_avg_loss': 0.6515940636396408, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=0.107]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 74, 'epoch_avg_loss': 0.7196678414940834, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.0718]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 75, 'epoch_avg_loss': 0.6459281474351883, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/100: 100%|██████████| 15/15 [01:02<00:00,  4.15s/it, loss=0.614]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 76, 'epoch_avg_loss': 0.6141646206378937, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.117]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 77, 'epoch_avg_loss': 0.6053769990801812, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/100: 100%|██████████| 15/15 [01:02<00:00,  4.16s/it, loss=0.0906]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 78, 'epoch_avg_loss': 0.481364772717158, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.367]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 79, 'epoch_avg_loss': 0.6028109629948933, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/100: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it, loss=0.053]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 80, 'epoch_avg_loss': 0.5481070970495542, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/100: 100%|██████████| 15/15 [01:02<00:00,  4.15s/it, loss=0.311]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 81, 'epoch_avg_loss': 0.58655673066775, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/100: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it, loss=0.0566]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 82, 'epoch_avg_loss': 0.4674671803911527, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/100: 100%|██████████| 15/15 [01:02<00:00,  4.15s/it, loss=0.0646]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 83, 'epoch_avg_loss': 0.4921173363924026, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/100: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it, loss=1.03] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 84, 'epoch_avg_loss': 0.5313890526692072, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86/100: 100%|██████████| 15/15 [01:01<00:00,  4.09s/it, loss=0.0375]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 85, 'epoch_avg_loss': 0.42770294323563574, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87/100: 100%|██████████| 15/15 [01:02<00:00,  4.17s/it, loss=0.305]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 86, 'epoch_avg_loss': 0.46677924593289694, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88/100: 100%|██████████| 15/15 [01:01<00:00,  4.08s/it, loss=0.168]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 87, 'epoch_avg_loss': 0.464057727654775, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89/100: 100%|██████████| 15/15 [01:02<00:00,  4.16s/it, loss=0.202]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 88, 'epoch_avg_loss': 0.3746654709180196, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90/100: 100%|██████████| 15/15 [01:01<00:00,  4.13s/it, loss=0.145]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 89, 'epoch_avg_loss': 0.6090888182322184, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=0.361]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 90, 'epoch_avg_loss': 0.7034722159306208, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92/100: 100%|██████████| 15/15 [01:02<00:00,  4.13s/it, loss=0.16] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 91, 'epoch_avg_loss': 0.5086625476678213, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93/100: 100%|██████████| 15/15 [01:00<00:00,  4.06s/it, loss=0.28] \n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 92, 'epoch_avg_loss': 0.5281831194957097, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94/100: 100%|██████████| 15/15 [01:01<00:00,  4.12s/it, loss=0.339]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 93, 'epoch_avg_loss': 0.6010710914929708, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=0.149]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 94, 'epoch_avg_loss': 0.5162178695201873, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96/100: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it, loss=0.0469]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 95, 'epoch_avg_loss': 0.4447437519828478, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97/100: 100%|██████████| 15/15 [01:02<00:00,  4.14s/it, loss=0.0687]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 96, 'epoch_avg_loss': 0.48555296858151753, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98/100: 100%|██████████| 15/15 [01:00<00:00,  4.03s/it, loss=0.0199]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 97, 'epoch_avg_loss': 0.5193856744716565, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99/100: 100%|██████████| 15/15 [01:01<00:00,  4.10s/it, loss=0.0364]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 98, 'epoch_avg_loss': 0.38644389708836874, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100/100: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it, loss=0.149]\n","output_type":"stream"},{"name":"stdout","text":"{'epoch': 99, 'epoch_avg_loss': 0.5174178719520569, 'batch_accuracy': 1.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>▁▃▃▅▅▅▅▅▆▅▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_avg_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_accuracy</td><td>1.0</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>epoch_avg_loss</td><td>0.51742</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">full_twinkle_bsz5_lr0.0001</strong> at: <a href='https://wandb.ai/elles/video2music/runs/58qujzdf' target=\"_blank\">https://wandb.ai/elles/video2music/runs/58qujzdf</a><br/> View project at: <a href='https://wandb.ai/elles/video2music' target=\"_blank\">https://wandb.ai/elles/video2music</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240517_150234-58qujzdf/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  inputs = {\n        \"encoder_outputs\": last_hidden_states.to('cuda'),\n        \"decoder_input_ids\": labels.unsqueeze(0)\n        # \"attention_mask\": batch[\"attention_mask\"].to('cuda'),\n        # \"labels\": batch[\"decoder_input_ids\"].to('cuda'), # TODO: this for sure gets shifted automatically by the library no?\n  }\n  outputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:20:25.925933Z","iopub.execute_input":"2024-05-17T12:20:25.926879Z","iopub.status.idle":"2024-05-17T12:20:25.945260Z","shell.execute_reply.started":"2024-05-17T12:20:25.926839Z","shell.execute_reply":"2024-05-17T12:20:25.944177Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Check that the trained model is outputting correct sequence","metadata":{}},{"cell_type":"code","source":"compute_accuracy(outputs, labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:28:17.024637Z","iopub.execute_input":"2024-05-17T12:28:17.025018Z","iopub.status.idle":"2024-05-17T12:28:17.032163Z","shell.execute_reply.started":"2024-05-17T12:28:17.024987Z","shell.execute_reply":"2024-05-17T12:28:17.031156Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}]},{"cell_type":"code","source":"  logits = outputs.logits\n  predicted_ids = torch.argmax(logits, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:21.314149Z","iopub.execute_input":"2024-05-17T12:21:21.315011Z","iopub.status.idle":"2024-05-17T12:21:21.338984Z","shell.execute_reply.started":"2024-05-17T12:21:21.314976Z","shell.execute_reply":"2024-05-17T12:21:21.337743Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"predicted_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:24.918999Z","iopub.execute_input":"2024-05-17T12:21:24.919626Z","iopub.status.idle":"2024-05-17T12:21:24.926423Z","shell.execute_reply.started":"2024-05-17T12:21:24.919588Z","shell.execute_reply":"2024-05-17T12:21:24.925467Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[  4, 189, 308,  32, 109, 140,  44, 110, 134, 199,  44, 111, 130]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:34.861775Z","iopub.execute_input":"2024-05-17T12:21:34.862698Z","iopub.status.idle":"2024-05-17T12:21:34.868828Z","shell.execute_reply.started":"2024-05-17T12:21:34.862659Z","shell.execute_reply":"2024-05-17T12:21:34.867915Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([  4, 189, 308,  32, 109, 140,  44, 110, 134, 199,  44, 111, 130],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"# tokens = midi_tokens[batch_i]\n# tokenss.append(tokens)\n# midi = tokenizer([tokens])\n# midis.append(midi)\n# midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n# midi_translations.append(midi_translation)","metadata":{},"execution_count":null,"outputs":[]}]}