{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8208644,"sourceType":"datasetVersion","datasetId":4864249},{"sourceId":8449096,"sourceType":"datasetVersion","datasetId":5034891},{"sourceId":8449248,"sourceType":"datasetVersion","datasetId":5035017},{"sourceId":8449475,"sourceType":"datasetVersion","datasetId":5035182},{"sourceId":8451785,"sourceType":"datasetVersion","datasetId":5036904}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install av miditok py_midicsv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:47:37.119582Z","iopub.execute_input":"2024-05-18T16:47:37.120269Z","iopub.status.idle":"2024-05-18T16:47:54.197793Z","shell.execute_reply.started":"2024-05-18T16:47:37.120233Z","shell.execute_reply":"2024-05-18T16:47:54.196878Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting miditok\n  Downloading miditok-3.0.3-py3-none-any.whl.metadata (9.0 kB)\nCollecting py_midicsv\n  Downloading py_midicsv-4.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: huggingface-hub>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.22.2)\nRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from miditok) (1.26.4)\nCollecting symusic>=0.4.3 (from miditok)\n  Downloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.15.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from miditok) (4.66.1)\nRequirement already satisfied: rich-click<2.0.0,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from py_midicsv) (1.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (4.9.0)\nRequirement already satisfied: click>=7 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (8.1.7)\nRequirement already satisfied: rich>=10.7.0 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (13.7.0)\nCollecting pySmartDL (from symusic>=0.4.3->miditok)\n  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from symusic>=0.4.3->miditok) (4.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.16.4->miditok) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (2.17.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (2024.2.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (0.1.2)\nDownloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading miditok-3.0.3-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py_midicsv-4.0.0-py3-none-any.whl (16 kB)\nDownloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\nInstalling collected packages: pySmartDL, symusic, av, py_midicsv, miditok\nSuccessfully installed av-12.0.0 miditok-3.0.3 pySmartDL-1.3.4 py_midicsv-4.0.0 symusic-0.4.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import av\nimport numpy as np\nimport random\nfrom torchvision.utils import make_grid\n\nfrom transformers import VivitImageProcessor, VivitModel, VivitConfig, TransfoXLLMHeadModel, TransfoXLConfig\nfrom huggingface_hub import hf_hub_download\n# print(transformers.__version__)\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport py_midicsv as pm\nimport torch\nimport torch.nn.functional as F\n\nfrom PIL import Image\nimport os\nfrom miditok import REMI, TokenizerConfig  # here we choose to use REMI\nimport miditok\nfrom pathlib import Path\nfrom torch.cuda.amp import autocast\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data.dataloader import default_collate\nfrom torch import optim\n\nfrom tqdm import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:47:54.199885Z","iopub.execute_input":"2024-05-18T16:47:54.200243Z","iopub.status.idle":"2024-05-18T16:48:10.073589Z","shell.execute_reply.started":"2024-05-18T16:47:54.200210Z","shell.execute_reply":"2024-05-18T16:48:10.072714Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-18 16:48:00.928719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-18 16:48:00.928822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-18 16:48:01.009578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# elle\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef cfg_to_dict(cfg):\n    return {attr: getattr(cfg, attr) for attr in dir(cfg) if not attr.startswith(\"__\") and not callable(getattr(cfg, attr))}\n\ndef read_frames_from_path(frames_path, indices, rgb=False):\n    '''\n    Read specific frames from a directory containing image files of video frames.\n    Args:\n        frames_path (str): Path to the directory containing frame images.\n        indices (List[int]): List of frame indices to read.\n    Returns:\n        result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n    '''\n    # List all files in the directory and sort them to maintain order\n    all_files = sorted(os.listdir(frames_path))\n    frames = []\n\n    # Process only files at specific indices\n    color_mode = 'RGB' if rgb else 'L'\n    for idx in indices:\n        if idx < len(all_files):\n            file_path = os.path.join(frames_path, all_files[idx])\n            with Image.open(file_path) as img:\n                # Convert image to RGB to ensure consistency\n                img = img.convert(color_mode)\n                # Calculate differences to make the image square\n                width, height = img.size\n                max_side = max(width, height)\n                # Create a new image with a black background\n                new_img = Image.new(color_mode, (max_side, max_side))\n                # Paste the original image onto the center of the new image\n                new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                frame_array = np.array(new_img)\n                if color_mode == 'L':\n                    # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                    frame_array = np.expand_dims(frame_array, axis=-1)\n                frames.append(frame_array)\n\n    stacked_frames = np.stack(frames, axis=0)\n    return stacked_frames\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:48:10.075277Z","iopub.execute_input":"2024-05-18T16:48:10.075595Z","iopub.status.idle":"2024-05-18T16:48:10.096434Z","shell.execute_reply.started":"2024-05-18T16:48:10.075567Z","shell.execute_reply":"2024-05-18T16:48:10.095494Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n\nclass Video2MIDIDataset(Dataset):\n    def __init__(self, root_dir, tokenizer, image_processor, transform=None, color_mode='gray'):\n        self.root_dir = root_dir\n        self.frames_dir = os.path.join(root_dir, 'frames')\n        self.midi_dir = os.path.join(root_dir, 'midi')\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.transform = transform\n        self.piece_names = [d for d in os.listdir(self.frames_dir) if os.path.isdir(os.path.join(self.frames_dir, d))]\n#         self.piece_names = self.piece_names[:2] # TODO: remove!!!\n        assert self.piece_names, f\"frame_dir at {self.frames_dir} is empty!\"\n        self.color_mode = 'L' if color_mode == 'gray' else 'RGB'\n        # for when we want to train on different sampling rate than what data has\n        # e.g. if we have 64 frames but want only 32 of them every 2 frames\n        self.frame_indices = set(sample_frame_indices(32, frame_sample_rate=2, seg_len=65))\n        assert len(self.frame_indices) == 32\n\n    def __len__(self):\n        return len(self.piece_names)\n\n    def __getitem__(self, idx):\n        piece_name = self.piece_names[idx]\n        frames_path = os.path.join(self.frames_dir, piece_name)\n        midi_path = os.path.join(self.midi_dir, f'{piece_name}.mid')\n\n        midi_token_ids = self.load_midi(midi_path)\n        frames = self.load_frames(frames_path, rgb=True)\n#         img_side_width = 64\n#         img_size = (img_side_width,img_side_width)\n#         print(frames.shape, type(frames))\n        # NOTE (elle): MUST convert frames to list for some reason otherwise it complains!!!\n#         processed_frames = self.image_processor(list(frames), return_tensors=\"pt\", do_center_crop=False, do_resize=True, size=img_size)\n        processed_frames = image_processor(list(frames), return_tensors=\"pt\")\n#         print(processed_frames)\n\n        sample = {'frames': processed_frames['pixel_values'], 'midi_tokens': midi_token_ids}\n        return sample\n\n#     def load_frames(self, frames_path):\n#         frame_files = sorted(os.listdir(frames_path))\n#         frames = [Image.open(os.path.join(frames_path, f)).convert(self.color_mode) for f in frame_files]\n#         print(f\"img shape: {print(frames[0].size)}\")\n#         if self.transform:\n#             frames = [self.transform(frame) for frame in frames]\n#         return frames\n\n    def load_frames(self, frames_path, rgb=False):\n        '''\n        Read specific frames from a directory containing image files of video frames.\n        Args:\n            frames_path (str): Path to the directory containing frame images.\n            indices (List[int]): List of frame indices to read.\n        Returns:\n            result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n        '''\n        # List all files in the directory and sort them to maintain order\n        frame_names = sorted(os.listdir(frames_path))\n        frames = []\n\n        # Process only files at specific indices\n        color_mode = 'RGB' if rgb else 'L'\n        for idx, frame_name in enumerate(frame_names):\n            if self.frame_indices == {} or (self.frame_indices != {} and idx in self.frame_indices):\n                file_path = f\"{frames_path}/{frame_name}\"\n                with Image.open(file_path) as img:\n                    # Convert image to RGB to ensure consistency\n                    img = img.convert(color_mode)\n                    # Calculate differences to make the image square\n                    width, height = img.size\n                    max_side = max(width, height)\n                    # Create a new image with a black background\n                    new_img = Image.new(color_mode, (max_side, max_side))\n                    # Paste the original image onto the center of the new image\n                    new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                    frame_array = np.array(new_img)\n                    if color_mode == 'L':\n                        # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                        frame_array = np.expand_dims(frame_array, axis=-1)\n                    frames.append(frame_array)\n        stacked_frames = np.stack(frames, axis=0)\n        return stacked_frames\n\n    def load_midi(self, midi_path):\n#         midi_token_ids_numpy = np.array(self.tokenizer(midi_path)[0].ids)\n        midi_tokens = self.tokenizer(midi_path)\n        midi_token_ids = torch.tensor(midi_tokens[0].ids, dtype=torch.long)\n        return midi_token_ids\n\ndef custom_collate_fn(batch, tokenizer):\n    # Extract frames and midi_tokens from the batch\n    frames = [item['frames'] for item in batch]\n    midi_tokens = [item['midi_tokens'] for item in batch]\n    # Pad the midi_tokens\n    # Assuming tokenizer provides PAD token index via tokenizer['PAD_None']\n    pad_token_index = tokenizer['PAD_None']  # Ensure this is the correct index for your PAD token\n    # print length before padding\n#     print(\"Length before padding: \", [len(midi_token) for midi_token in midi_tokens])\n    midi_tokens_padded = pad_sequence(midi_tokens, batch_first=True, padding_value=pad_token_index)\n#     print(\"Length after padding: \", [len(midi_token) for midi_token in midi_tokens_padded])\n\n    # Collate frames normally (assuming they are tensors of the same shape)\n    frames = default_collate(frames)\n    # Return a new dictionary with padded midi_tokens and frames\n    return {'frames': frames, 'midi_tokens': midi_tokens_padded}\n\ndef show_images_and_midi(dataloader):\n    for i, batch in enumerate(dataloader):\n        frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n        midi_tokens = batch['midi_tokens']  # MIDI tokens\n\n        print(f\"Batch {i + 1}\")\n\n        # Calculate the number of rows and columns for the subplots\n        batch_size = frames.size(0) # torch.Size([4, 1, 302, 1, 64, 64])\n#         n_igms_in_batch = frames.size(2)\n        cols = int(np.ceil(np.sqrt(batch_size)))\n        rows = int(np.ceil(batch_size / cols))\n\n        # Displaying images in a grid that's as square as possible\n        fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n        axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n        for j in range(batch_size):\n            # Select the first image from the video sequence and remove the singleton dimensions\n            img = frames[j, 0, 0, 0]  # Reduces to (64, 64)\n            axs[j].imshow(img.numpy())\n            axs[j].axis('off')  # Hide axes\n            axs[j].set_title(f'MIDI: {midi_tokens[j]}')  # Optionally print MIDI token IDs\n\n        # Hide any unused axes if the total number of subplots exceeds the batch size\n        for k in range(batch_size, len(axs)):\n            axs[k].axis('off')\n\n        plt.show()\n\n        # Optional: stop after first batch for demonstration\n        if i == 0:\n            break\n\ndef show_frames(frames):\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = len(frames)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[j]  # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.numpy())\n        axs[j].axis('off')  # Hide axes\n        axs[j].set_title(f'MIDI: {midi_tokens[0]}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    plt.show()\n\ndef show_images_and_midi_one_batch(dataloader, tokenizer):\n    # Fetch the first batch from the dataloader\n    token_id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n    batch = nextclas(iter(dataloader))\n    batch_i = 0\n    frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n    # ^ [4, 1, 32, 3, 224, 224]\n    midi_tokens = batch['midi_tokens']  # MIDI tokens\n    print(f\"miditokens shape {midi_tokens.shape}\")\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = frames.size(2)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    midi_translations = []\n    midis = []\n    tokenss = []\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[0, 0, j] # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.permute(1, 2, 0).numpy())\n        axs[j].axis('off')  # Hide axes\n        tokens = midi_tokens[batch_i]\n        tokenss.append(tokens)\n        midi = tokenizer([tokens])\n        midis.append(midi)\n        midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n        midi_translations.append(midi_translation)\n#         axs[j].set_title(f'MIDI: {midi_tokens_translated}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    title = f\"{midis[batch_i]}\\n{midi_translations[batch_i]}\\n{tokenss[batch_i]}\"\n    fig.suptitle(title, fontsize=16)\n    print(title)\n\n    plt.show()\n\ndef collate_fn(batch):\n    return custom_collate_fn(batch, tokenizer)\n\ndef compute_accuracy(outputs, labels):\n    logits = outputs.logits\n    prediction_ids = torch.argmax(logits, dim=-1)\n    # Flatten the tensors to compare each token\n    prediction_ids = prediction_ids.view(-1)\n    labels = labels.view(-1)\n    \n    # Compare predictions with labels\n    correct = (prediction_ids == labels).sum().item()\n    total = labels.size(0)\n\n    accuracy = correct / total\n    return accuracy, correct, total","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:28:11.465189Z","iopub.execute_input":"2024-05-18T17:28:11.465618Z","iopub.status.idle":"2024-05-18T17:28:11.516873Z","shell.execute_reply.started":"2024-05-18T17:28:11.465574Z","shell.execute_reply":"2024-05-18T17:28:11.515216Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# TODO\n* investigate why this training loop (using real data but just twinkle sequence 0, one second) seems to work now -- is it AdamW, the LR, or probably just that getting a transformer to overfit on random labels won't work because not properly padded with start/end token? although idk bc I feel like I tried it with non-random at first and it didn't work so truly I have no idea\n* why does the loss not quite go to 0 though? It hovers around .03 and then is a bit unstable and goes to .3 even sometimes e.g. a continuation of it although btw here https://wandb.ai/elles/video2music/runs/kmfa7au7?nw=nwuserellesummer I would think it should go to .0005ish which is what happened when I overfit to a single label sequence of the same numbers although at one point it did get to 0.0086 (not tracked in wandb). Just feel like it should be trivially easy/stable to memorize one one-sec sequence idk\n* BUT good news: despite loss not being ~0, the output of the model is still the given sequence, so all is good heh\n* ^^ take it further and track wer metric in general AH or just the accuracy tbh like per token (if it doesn't match for example)\n\nbtw for original model/dataset (not ours! aka the audio one)\nshapes:\nlabels: torch.Size([1, 67]). encoder_outputs: torch.Size([1, 112, 768]). labels: torch.Size([1, 22]). encoder_outputs: torch.Size([1, 113, 768]).\n\nour model tho is like: encoder_outputs: torch.Size([1, 3137, 768]) labels: torch.Size([1, 16])  \nwith 2 batch size  \nlabel shape torch.Size([2, 20])  \nframe shape torch.Size([2, 33, 3, 224, 224])  \nencoder_outputs shape torch.Size([2, 3137, 768])  ","metadata":{}},{"cell_type":"markdown","source":"## NEXT\n* last progress: able to fully overfit on twinkle 1 second clips but really slowly (converged to 100% accuracy after an hour yikes)\n* is slowness bc of the size of the encoder? is this worth the trade off of not having to train our own encoder? or is it bc the image quality has a lot of noise when resizing to fit the encoder needs? maybe worth using Khalil's modified encoder then...\n* will the model scale well to e.g. 2-5 second clips? (dimitrios suggested 5 sec) and across different songs?\n* try creating validation set just within twinkle twinkle? how to create a good one for this task in general? e.g. what should be \"allowed\" to overlap? We should see much less overlap in both inp/output when using longer sequences e.g. due to permutation diversity\n* debug why the original training didn't work for your own sanity/learning","metadata":{}},{"cell_type":"code","source":"# frames_path = f\"{ds_dir}/processed/frames/raw--twinke_twinkle_0\"\n# # indices = sample_frame_indices(clip_len=32, frame_sample_rate=1)\n# indices = range(32)\n# video = read_frames_from_path(frames_path=frames_path, indices=indices, rgb=True)\n# inputs = image_processor(list(video), return_tensors=\"pt\")\n# print(inputs[\"pixel_values\"].shape)\n# fig, ax = plt.subplots(2, 6, figsize=(12, 8))\n# for i in range(12):\n#     ax[i // 3, i % 3].imshow(inputs[\"pixel_values\"][0, i].permute(1, 2, 0).numpy())\n#     ax[i // 3, i % 3].axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:22:23.393504Z","iopub.execute_input":"2024-05-18T17:22:23.394222Z","iopub.status.idle":"2024-05-18T17:22:23.400450Z","shell.execute_reply.started":"2024-05-18T17:22:23.394191Z","shell.execute_reply":"2024-05-18T17:22:23.399093Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:22:23.883469Z","iopub.execute_input":"2024-05-18T17:22:23.883853Z","iopub.status.idle":"2024-05-18T17:22:23.890180Z","shell.execute_reply.started":"2024-05-18T17:22:23.883814Z","shell.execute_reply":"2024-05-18T17:22:23.889084Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def show_data_samples(data_loader, title, nrow=3, figsize=(15, 6)):\n    # Get a batch of training data\n#     idx2class = {0: 'A', 1: 'Ab', 2: 'B', 3: 'Bb', 4: 'C', 5: 'D', 6: 'E', 7: 'Eb', 8: 'F', 9: 'G', 10: 'Gb'}\n    dataiter = iter(data_loader)\n    batch = next(dataiter)\n    # just show first seq\n    frames, labels = batch[\"frames\"][0][0], batch[\"midi_tokens\"][0]\n    # subset does not have class_to_idx so remove this line\n    # idx2class = {v: k for k, v in data_loader.dataset.class_to_idx.items()}\n#     print(idx2class)\n    print(labels)\n    label_list = [label.item() for label in labels]\n    print(labels)\n#     notes = [idx2class[label] for label in label_list]\n\n    plt.figure(figsize=figsize)\n    plt.title(f\"{title} {label_list}\")\n    print(frames.shape)\n    imshow(make_grid(frames[:10], nrow=nrow))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:36:56.448077Z","iopub.execute_input":"2024-05-18T13:36:56.448421Z","iopub.status.idle":"2024-05-18T13:36:56.456891Z","shell.execute_reply.started":"2024-05-18T13:36:56.448393Z","shell.execute_reply":"2024-05-18T13:36:56.455841Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"show_data_samples(DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn), 'Training Data Samples', figsize=(20,20))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:36:57.431658Z","iopub.execute_input":"2024-05-18T13:36:57.432009Z","iopub.status.idle":"2024-05-18T13:36:57.475092Z","shell.execute_reply.started":"2024-05-18T13:36:57.431982Z","shell.execute_reply":"2024-05-18T13:36:57.474022Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_data_samples(DataLoader(\u001b[43mdataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Data Samples\u001b[39m\u001b[38;5;124m'\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from transformers import SpeechT5ForSpeechToText, SpeechT5Config\nfrom datasets import load_metric\n\n# ds_dir = \"../input/data-32fps/data_32fps\"\n# ds_dir = \"/kaggle/input/twinkle-32fps-5s/Users/ellemcfarlane/Documents/dtu/ai-synth/data_32fps_5s\"\n# ds_dir = \"/kaggle/input/data-13fps-5s/data_13fps_5s\"\nds_dir = \"/kaggle/input/pop-20/data_pop\"\n# frames_path = f\"{ds_dir}/processed/frames/somebody--twinke_twinkle_0\"\n# frames_path = f\"{ds_dir}/processed/frames/raw--twinke_twinkle_0\"\n# Our parameters\nTOKENIZER_PARAMS = {\n    \"pitch_range\": (21, 109),\n    \"beat_res\": {(0, 4): 8, (4, 12): 4}, # TODO: finetune beat res?\n    \"num_velocities\": 32,\n    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n    \"use_chords\": True,\n    \"use_rests\": False,\n    \"use_tempos\": True,\n    \"use_time_signatures\": False,\n    \"use_programs\": False,\n    \"num_tempos\": 32,  # number of tempo bins\n    \"tempo_range\": (40, 250),  # (min, max)\n}\ntok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n# wer_metric = load_metric(\"wer\")\n# Creates the tokenizer\ntokenizer = REMI(tok_config)\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\ntrain_dataset = Video2MIDIDataset(\n    root_dir=f\"{ds_dir}/processed/train\",\n    tokenizer=tokenizer,\n    image_processor=image_processor\n    # transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n)\nval_dataset = Video2MIDIDataset(\n    root_dir=f\"{ds_dir}/processed/val\",\n    tokenizer=tokenizer,\n    image_processor=image_processor\n    # transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n)\nprint(f\"Training on {len(train_dataset.piece_names)}, validating on {len(val_dataset.piece_names)}\")\n\nconfig = SpeechT5Config(\n    vocab_size=328,\n    d_model=768,\n    max_length=450\n)\n\nmodel = SpeechT5ForSpeechToText(config)\nmodel = model.to('cuda')  # Ensure your model is on GPU if available\nmodel_enc = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\nmodel_enc = model_enc.to('cuda')\nmodel.speecht5.encoder = None\n\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 43\n    \n    LOG_EVERY_X_EPOCHS = 1\n#     SAVE_EVERY_X_EPOCHS = 10\n\n    LR = 0.0001 #5e-5 # 4.786300923226385e-05 # 0.0001\n    EPOCHS = 100\n    BATCH_SIZE = 6\n\n    USE_WANDB = True\n    WANDB_PROJECT = \"video2music\"\n    WANDB_ENTITY = \"elles\"\n    WANDB_GROUP = f\"pop20_bsz{BATCH_SIZE}_lr{LR}\"\n    EXPERIMENT = WANDB_GROUP\n    # FPS = 32\n\ncfg = Config()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn)\n\nif cfg.SEED:\n    set_seed(cfg.SEED)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR)\n# Training loop\nmodel.train()\n\nif cfg.USE_WANDB:\n    # convert cfg class to dict\n    cfg_dict = cfg_to_dict(cfg)\n    cfg_dict.update(config.to_dict())\n    print(cfg_dict)\n    assert cfg_dict != {}, \"cfg_dict is empty\"\n    wandb_id = wandb.util.generate_id()\n    wandb.init(\n        project=cfg.WANDB_PROJECT,\n        name=cfg.EXPERIMENT,\n        entity=cfg.WANDB_ENTITY,\n        config=cfg_dict,\n        id=wandb_id,\n        resume=\"allow\",\n        group=cfg.WANDB_GROUP\n    )\n    wandb.watch(model, log=\"all\", log_freq=10)\n\nepochs = cfg.EPOCHS\n\nbest_acc = 0\nfor epoch in range(epochs):\n    total_correct = 0\n    total_tokens = 0\n    total_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}')\n    model.train()\n    for batch in progress_bar:\n  \n        optimizer.zero_grad()\n        frames = batch['frames'].to('cuda')\n        labels = batch['midi_tokens'].to('cuda')\n        with torch.no_grad():\n#             frames = batch['frames']\n\n            # labels = rand_labels.unsqueeze(0)\n#             labels = batch['midi_tokens']\n#             frames = frames.to('cuda')\n            # squeeze only if batch-size is > 1\n            frames = frames.squeeze(1)\n#             labels = labels.to('cuda')\n            outputs = model_enc(frames)\n            last_hidden_states = outputs.last_hidden_state\n#         new_encoder_outputs = last_hidden_states.to('cuda')\n        new_encoder_outputs = (last_hidden_states,)\n        inputs = {\n            \"encoder_outputs\": new_encoder_outputs,\n            \"labels\": labels # labels.unsqueeze(0) only unsqueeze if batch size is 1?\n        }\n        outputs = model(**inputs)\n\n        # Loss computation\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n        accuracy, batch_corr, batch_tot = compute_accuracy(outputs, labels)\n\n        # Accumulate results\n        # TODO fix: this is not accumulating per batch..\n        total_correct += batch_corr\n        total_tokens += batch_tot\n\n        progress_bar.set_postfix({'loss': loss.item()})\n\n    # Logging to wandb\n    average_loss_train = total_loss / len(train_dataloader)\n    overall_train_accuracy = total_correct / total_tokens\n    stats = {'epoch': epoch, 'train_loss': average_loss_train, 'train_accuracy': overall_train_accuracy}\n    if epoch % cfg.LOG_EVERY_X_EPOCHS == 0:\n        total_correct_val = 0\n        total_tokens_val = 0\n        total_loss_val = 0\n        model.eval()\n        with torch.no_grad():\n            progress_bar = tqdm(val_dataloader)\n            for batch in progress_bar:\n                frames = batch['frames'].to('cuda')\n                labels = batch['midi_tokens'].to('cuda')\n                frames = frames.squeeze(1)\n                outputs = model_enc(frames)\n                last_hidden_states = outputs.last_hidden_state\n                new_encoder_outputs = (last_hidden_states,)\n                inputs = {\n                    \"encoder_outputs\": new_encoder_outputs,\n                    \"labels\": labels # labels.unsqueeze(0) only unsqueeze if batch size is 1?\n                }\n                outputs = model(**inputs)\n                loss = outputs.loss\n                total_loss_val += loss.item()\n                _accuracy_val, batch_corr, batch_tot = compute_accuracy(outputs, labels)\n\n                # Accumulate results\n                # TODO fix: this is not accumulating per batch..\n                total_correct_val += batch_corr\n                total_tokens_val += batch_tot\n            average_loss_val = total_loss_val / len(val_dataloader)\n            overall_accuracy_val = total_correct_val / total_tokens_val\n            stats['val_loss'] = average_loss_val\n            stats['val_accuracy'] = overall_accuracy_val\n#             stats = {'epoch': epoch, 'val_loss': average_loss_val, 'val_accuracy': overall_accuracy_val}\n            print(stats)\n            if overall_accuracy_val > best_acc:\n                print(\"saving best model so far\")\n                torch.save(model.state_dict(), f\"{cfg.EXPERIMENT}.pt\")\n            if wandb.run:\n                wandb.log(stats)\n    del frames, labels, outputs, last_hidden_states, new_encoder_outputs, inputs\n    torch.cuda.empty_cache()\n\n# End of training\nif wandb.run:\n  wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:28:41.224219Z","iopub.execute_input":"2024-05-18T17:28:41.224560Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2907020809.py:24: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n  tok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n","output_type":"stream"},{"name":"stdout","text":"Training on 314, validating on 142\n","output_type":"stream"},{"name":"stderr","text":"Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'BATCH_SIZE': 6, 'DEVICE': 'cuda', 'EPOCHS': 100, 'EXPERIMENT': 'pop20_bsz6_lr0.0001', 'LOG_EVERY_X_EPOCHS': 1, 'LR': 0.0001, 'SEED': 43, 'USE_WANDB': True, 'WANDB_ENTITY': 'elles', 'WANDB_GROUP': 'pop20_bsz6_lr0.0001', 'WANDB_PROJECT': 'video2music', 'vocab_size': 328, 'hidden_size': 768, 'encoder_layers': 12, 'encoder_ffn_dim': 3072, 'encoder_attention_heads': 12, 'encoder_layerdrop': 0.1, 'decoder_layers': 6, 'decoder_ffn_dim': 3072, 'decoder_attention_heads': 12, 'decoder_layerdrop': 0.1, 'hidden_act': 'gelu', 'positional_dropout': 0.1, 'hidden_dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'scale_embedding': False, 'feat_extract_norm': 'group', 'feat_proj_dropout': 0.0, 'feat_extract_activation': 'gelu', 'conv_dim': [512, 512, 512, 512, 512, 512, 512], 'conv_stride': [5, 2, 2, 2, 2, 2, 2], 'conv_kernel': [10, 3, 3, 3, 3, 2, 2], 'conv_bias': False, 'num_conv_pos_embeddings': 128, 'num_conv_pos_embedding_groups': 16, 'num_feat_extract_layers': 7, 'apply_spec_augment': True, 'mask_time_prob': 0.05, 'mask_time_length': 10, 'mask_time_min_masks': 2, 'mask_feature_prob': 0.0, 'mask_feature_length': 10, 'mask_feature_min_masks': 0, 'num_mel_bins': 80, 'speech_decoder_prenet_layers': 2, 'speech_decoder_prenet_units': 256, 'speech_decoder_prenet_dropout': 0.5, 'speaker_embedding_dim': 512, 'speech_decoder_postnet_layers': 5, 'speech_decoder_postnet_units': 256, 'speech_decoder_postnet_kernel': 5, 'speech_decoder_postnet_dropout': 0.5, 'reduction_factor': 2, 'max_speech_positions': 4000, 'max_text_positions': 450, 'encoder_max_relative_position': 160, 'use_guided_attention_loss': True, 'guided_attention_loss_num_heads': 2, 'guided_attention_loss_sigma': 0.4, 'guided_attention_loss_scale': 10.0, 'use_cache': True, 'is_encoder_decoder': True, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 450, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': 2, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.39.3', 'd_model': 768, 'model_type': 'speecht5'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:st182x1c) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█▁▁▂</td></tr><tr><td>train_accuracy</td><td>▁▇▅▇██▇▁▁▇</td></tr><tr><td>train_loss</td><td>█▄▃▂▁▁▁██▄</td></tr><tr><td>val_accuracy</td><td>█▁███████▅</td></tr><tr><td>val_loss</td><td>▆▂▄▂▂▂▂▄█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.304</td></tr><tr><td>train_loss</td><td>4.31632</td></tr><tr><td>val_accuracy</td><td>0.02083</td></tr><tr><td>val_loss</td><td>5.62305</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pop20_bsz6_lr0.0001</strong> at: <a href='https://wandb.ai/elles/video2music/runs/st182x1c' target=\"_blank\">https://wandb.ai/elles/video2music/runs/st182x1c</a><br/> View project at: <a href='https://wandb.ai/elles/video2music' target=\"_blank\">https://wandb.ai/elles/video2music</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240518_171545-st182x1c/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:st182x1c). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240518_172845-7vhfnghq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/elles/video2music/runs/7vhfnghq' target=\"_blank\">pop20_bsz6_lr0.0001</a></strong> to <a href='https://wandb.ai/elles/video2music' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/elles/video2music' target=\"_blank\">https://wandb.ai/elles/video2music</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/elles/video2music/runs/7vhfnghq' target=\"_blank\">https://wandb.ai/elles/video2music/runs/7vhfnghq</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/100:   0%|          | 0/53 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  return torch.tensor(value)\n/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  return torch.tensor(value)\n/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  return torch.tensor(value)\n/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  return torch.tensor(value)\nEpoch 1/100:  57%|█████▋    | 30/53 [02:34<02:08,  5.58s/it, loss=3.85]","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  inputs = {\n        \"encoder_outputs\": last_hidden_states.to('cuda'),\n        \"decoder_input_ids\": labels.unsqueeze(0)\n        # \"attention_mask\": batch[\"attention_mask\"].to('cuda'),\n        # \"labels\": batch[\"decoder_input_ids\"].to('cuda'), # TODO: this for sure gets shifted automatically by the library no?\n  }\n  outputs = model(**inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check that the trained model is outputting correct sequence","metadata":{}},{"cell_type":"code","source":"compute_accuracy(outputs, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  logits = outputs.logits\n  predicted_ids = torch.argmax(logits, dim=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens = midi_tokens[batch_i]\n# tokenss.append(tokens)\n# midi = tokenizer([tokens])\n# midis.append(midi)\n# midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n# midi_translations.append(midi_translation)","metadata":{},"execution_count":null,"outputs":[]}]}