{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8208644,"sourceType":"datasetVersion","datasetId":4864249}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install av miditok py_midicsv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T11:34:31.706780Z","iopub.execute_input":"2024-05-17T11:34:31.707141Z","iopub.status.idle":"2024-05-17T11:34:49.116039Z","shell.execute_reply.started":"2024-05-17T11:34:31.707112Z","shell.execute_reply":"2024-05-17T11:34:49.115125Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting miditok\n  Downloading miditok-3.0.3-py3-none-any.whl.metadata (9.0 kB)\nCollecting py_midicsv\n  Downloading py_midicsv-4.0.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: huggingface-hub>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.22.2)\nRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from miditok) (1.26.4)\nCollecting symusic>=0.4.3 (from miditok)\n  Downloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from miditok) (0.15.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from miditok) (4.66.1)\nRequirement already satisfied: rich-click<2.0.0,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from py_midicsv) (1.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.16.4->miditok) (4.9.0)\nRequirement already satisfied: click>=7 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (8.1.7)\nRequirement already satisfied: rich>=10.7.0 in /opt/conda/lib/python3.10/site-packages (from rich-click<2.0.0,>=1.6.1->py_midicsv) (13.7.0)\nCollecting pySmartDL (from symusic>=0.4.3->miditok)\n  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from symusic>=0.4.3->miditok) (4.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.16.4->miditok) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (2.17.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.16.4->miditok) (2024.2.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.7.0->rich-click<2.0.0,>=1.6.1->py_midicsv) (0.1.2)\nDownloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading miditok-3.0.3-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py_midicsv-4.0.0-py3-none-any.whl (16 kB)\nDownloading symusic-0.4.7-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\nInstalling collected packages: pySmartDL, symusic, av, py_midicsv, miditok\nSuccessfully installed av-12.0.0 miditok-3.0.3 pySmartDL-1.3.4 py_midicsv-4.0.0 symusic-0.4.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import av\nimport numpy as np\nimport random\n\nfrom transformers import VivitImageProcessor, VivitModel, VivitConfig, TransfoXLLMHeadModel, TransfoXLConfig\nfrom huggingface_hub import hf_hub_download\n# print(transformers.__version__)\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport py_midicsv as pm\nimport torch\nimport torch.nn.functional as F\n\nfrom PIL import Image\nimport os\nfrom miditok import REMI, TokenizerConfig  # here we choose to use REMI\nimport miditok\nfrom pathlib import Path\nfrom torch.cuda.amp import autocast\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data.dataloader import default_collate\nfrom torch import optim\n\nfrom tqdm import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-17T11:34:49.117994Z","iopub.execute_input":"2024-05-17T11:34:49.118272Z","iopub.status.idle":"2024-05-17T11:35:09.049711Z","shell.execute_reply.started":"2024-05-17T11:34:49.118244Z","shell.execute_reply":"2024-05-17T11:35:09.048795Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-17 11:34:57.784573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-17 11:34:57.784683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-17 11:34:57.951377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# elle\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef cfg_to_dict(cfg):\n    return {attr: getattr(cfg, attr) for attr in dir(cfg) if not attr.startswith(\"__\") and not callable(getattr(cfg, attr))}\n\ndef read_frames_from_path(frames_path, indices, rgb=False):\n    '''\n    Read specific frames from a directory containing image files of video frames.\n    Args:\n        frames_path (str): Path to the directory containing frame images.\n        indices (List[int]): List of frame indices to read.\n    Returns:\n        result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n    '''\n    # List all files in the directory and sort them to maintain order\n    all_files = sorted(os.listdir(frames_path))\n    frames = []\n\n    # Process only files at specific indices\n    color_mode = 'RGB' if rgb else 'L'\n    for idx in indices:\n        if idx < len(all_files):\n            file_path = os.path.join(frames_path, all_files[idx])\n            with Image.open(file_path) as img:\n                # Convert image to RGB to ensure consistency\n                img = img.convert(color_mode)\n                # Calculate differences to make the image square\n                width, height = img.size\n                max_side = max(width, height)\n                # Create a new image with a black background\n                new_img = Image.new(color_mode, (max_side, max_side))\n                # Paste the original image onto the center of the new image\n                new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                frame_array = np.array(new_img)\n                if color_mode == 'L':\n                    # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                    frame_array = np.expand_dims(frame_array, axis=-1)\n                frames.append(frame_array)\n\n    stacked_frames = np.stack(frames, axis=0)\n    return stacked_frames\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\ndef sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices","metadata":{"execution":{"iopub.status.busy":"2024-05-17T11:42:48.702398Z","iopub.execute_input":"2024-05-17T11:42:48.703038Z","iopub.status.idle":"2024-05-17T11:42:48.718661Z","shell.execute_reply.started":"2024-05-17T11:42:48.703003Z","shell.execute_reply":"2024-05-17T11:42:48.717728Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Video2MIDIDataset(Dataset):\n    def __init__(self, root_dir, tokenizer, image_processor, transform=None, color_mode='gray'):\n        self.root_dir = root_dir\n        self.frames_dir = os.path.join(root_dir, 'frames')\n        self.midi_dir = os.path.join(root_dir, 'midi')\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.transform = transform\n        self.piece_names = [d for d in os.listdir(self.frames_dir) if os.path.isdir(os.path.join(self.frames_dir, d))]\n#         self.piece_names = self.piece_names[:2] # TODO: remove!!!\n        assert self.piece_names, f\"frame_dir at {self.frames_dir} is empty!\"\n        self.color_mode = 'L' if color_mode == 'gray' else 'RGB'\n\n    def __len__(self):\n        return len(self.piece_names)\n\n    def __getitem__(self, idx):\n        piece_name = self.piece_names[idx]\n        frames_path = os.path.join(self.frames_dir, piece_name)\n        midi_path = os.path.join(self.midi_dir, f'{piece_name}.mid')\n\n        midi_token_ids = self.load_midi(midi_path)\n        frames = self.load_frames(frames_path, rgb=True)\n#         img_side_width = 64\n#         img_size = (img_side_width,img_side_width)\n#         print(frames.shape, type(frames))\n        # NOTE (elle): MUST convert frames to list for some reason otherwise it complains!!!\n#         processed_frames = self.image_processor(list(frames), return_tensors=\"pt\", do_center_crop=False, do_resize=True, size=img_size)\n        processed_frames = image_processor(list(frames), return_tensors=\"pt\")\n#         print(processed_frames)\n\n        sample = {'frames': processed_frames['pixel_values'], 'midi_tokens': midi_token_ids}\n        return sample\n\n#     def load_frames(self, frames_path):\n#         frame_files = sorted(os.listdir(frames_path))\n#         frames = [Image.open(os.path.join(frames_path, f)).convert(self.color_mode) for f in frame_files]\n#         print(f\"img shape: {print(frames[0].size)}\")\n#         if self.transform:\n#             frames = [self.transform(frame) for frame in frames]\n#         return frames\n\n    def load_frames(self, frames_path, rgb=False):\n        '''\n        Read specific frames from a directory containing image files of video frames.\n        Args:\n            frames_path (str): Path to the directory containing frame images.\n            indices (List[int]): List of frame indices to read.\n        Returns:\n            result (np.ndarray): numpy array of frames of shape (num_frames, height, width, 3).\n        '''\n        # List all files in the directory and sort them to maintain order\n        frame_names = sorted(os.listdir(frames_path))\n        frames = []\n\n        # Process only files at specific indices\n        color_mode = 'RGB' if rgb else 'L'\n        for frame_name in frame_names:\n            file_path = f\"{frames_path}/{frame_name}\"\n            with Image.open(file_path) as img:\n                # Convert image to RGB to ensure consistency\n                img = img.convert(color_mode)\n                # Calculate differences to make the image square\n                width, height = img.size\n                max_side = max(width, height)\n                # Create a new image with a black background\n                new_img = Image.new(color_mode, (max_side, max_side))\n                # Paste the original image onto the center of the new image\n                new_img.paste(img, ((max_side - width) // 2, (max_side - height) // 2))\n                frame_array = np.array(new_img)\n                if color_mode == 'L':\n                    # Expand dims to add the channel dimension, resulting in (H, W, 1)\n                    frame_array = np.expand_dims(frame_array, axis=-1)\n                frames.append(frame_array)\n\n        stacked_frames = np.stack(frames, axis=0)\n        return stacked_frames\n\n    def load_midi(self, midi_path):\n        midi_tokens = self.tokenizer(midi_path)\n        midi_token_ids = torch.tensor(midi_tokens[0].ids, dtype=torch.long)\n        return midi_token_ids\n\ndef custom_collate_fn(batch, tokenizer):\n    # Extract frames and midi_tokens from the batch\n    frames = [item['frames'] for item in batch]\n    midi_tokens = [item['midi_tokens'] for item in batch]\n\n    # Pad the midi_tokens\n    # Assuming tokenizer provides PAD token index via tokenizer['PAD_None']\n    pad_token_index = tokenizer['PAD_None']  # Ensure this is the correct index for your PAD token\n    # print length before padding\n#     print(\"Length before padding: \", [len(midi_token) for midi_token in midi_tokens])\n    midi_tokens_padded = pad_sequence(midi_tokens, batch_first=True, padding_value=pad_token_index)\n#     print(\"Length after padding: \", [len(midi_token) for midi_token in midi_tokens_padded])\n\n    # Collate frames normally (assuming they are tensors of the same shape)\n    frames = default_collate(frames)\n\n    # Return a new dictionary with padded midi_tokens and frames\n    return {'frames': frames, 'midi_tokens': midi_tokens_padded}\n\ndef show_images_and_midi(dataloader):\n    for i, batch in enumerate(dataloader):\n        frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n        midi_tokens = batch['midi_tokens']  # MIDI tokens\n\n        print(f\"Batch {i + 1}\")\n\n        # Calculate the number of rows and columns for the subplots\n        batch_size = frames.size(0) # torch.Size([4, 1, 302, 1, 64, 64])\n#         n_igms_in_batch = frames.size(2)\n        cols = int(np.ceil(np.sqrt(batch_size)))\n        rows = int(np.ceil(batch_size / cols))\n\n        # Displaying images in a grid that's as square as possible\n        fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n        axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n        for j in range(batch_size):\n            # Select the first image from the video sequence and remove the singleton dimensions\n            img = frames[j, 0, 0, 0]  # Reduces to (64, 64)\n            axs[j].imshow(img.numpy())\n            axs[j].axis('off')  # Hide axes\n            axs[j].set_title(f'MIDI: {midi_tokens[j]}')  # Optionally print MIDI token IDs\n\n        # Hide any unused axes if the total number of subplots exceeds the batch size\n        for k in range(batch_size, len(axs)):\n            axs[k].axis('off')\n\n        plt.show()\n\n        # Optional: stop after first batch for demonstration\n        if i == 0:\n            break\n\ndef show_frames(frames):\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = len(frames)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[j]  # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.numpy())\n        axs[j].axis('off')  # Hide axes\n        axs[j].set_title(f'MIDI: {midi_tokens[0]}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    plt.show()\n\ndef show_images_and_midi_one_batch(dataloader, tokenizer):\n    # Fetch the first batch from the dataloader\n    token_id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n    batch = nextclas(iter(dataloader))\n    batch_i = 0\n    frames = batch['frames']  # Assuming frames are tensors of shape (batch_size, channels, extra_dim, another_channel_like, height, width)\n    # ^ [4, 1, 32, 3, 224, 224]\n    midi_tokens = batch['midi_tokens']  # MIDI tokens\n    print(f\"miditokens shape {midi_tokens.shape}\")\n    print(\"First Batch\")\n\n    # Calculate the total number of images to display\n    total_images = frames.size(2)  # Assuming 'extra_dim' holds 32 images\n\n    cols = int(np.ceil(np.sqrt(total_images)))\n    rows = int(np.ceil(total_images / cols))\n\n    # Displaying images in a grid that's as square as possible\n    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))  # Adjust size as needed\n    axs = axs.flatten()  # Flatten the array of axes to make indexing easier\n\n    midi_translations = []\n    midis = []\n    tokenss = []\n    for j in range(total_images):\n        # Select the image from the video sequence for each frame in the batch\n        img = frames[0, 0, j] # Adjust indexing based on your data's shape, using the first item in batch\n        axs[j].imshow(img.permute(1, 2, 0).numpy())\n        axs[j].axis('off')  # Hide axes\n        tokens = midi_tokens[batch_i]\n        tokenss.append(tokens)\n        midi = tokenizer([tokens])\n        midis.append(midi)\n        midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n        midi_translations.append(midi_translation)\n#         axs[j].set_title(f'MIDI: {midi_tokens_translated}')  # Optionally print MIDI token IDs for the first item in batch\n\n    # Hide any unused axes if the total number of subplots exceeds the total images\n    for k in range(total_images, len(axs)):\n        axs[k].axis('off')\n\n    title = f\"{midis[batch_i]}\\n{midi_translations[batch_i]}\\n{tokenss[batch_i]}\"\n    fig.suptitle(title, fontsize=16)\n    print(title)\n\n    plt.show()\n\n# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\ndef collate_fn(batch):\n    return custom_collate_fn(batch, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T13:57:00.436560Z","iopub.execute_input":"2024-05-17T13:57:00.436964Z","iopub.status.idle":"2024-05-17T13:57:00.473771Z","shell.execute_reply.started":"2024-05-17T13:57:00.436924Z","shell.execute_reply":"2024-05-17T13:57:00.472773Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# TODO\n* investigate why this training loop (using real data but just twinkle sequence 0, one second) seems to work now -- is it AdamW, the LR, or probably just that getting a transformer to overfit on random labels won't work because not properly padded with start/end token? although idk bc I feel like I tried it with non-random at first and it didn't work so truly I have no idea\n* why does the loss not quite go to 0 though? It hovers around .03 and then is a bit unstable and goes to .3 even sometimes e.g. a continuation of it although btw here https://wandb.ai/elles/video2music/runs/kmfa7au7?nw=nwuserellesummer I would think it should go to .0005ish which is what happened when I overfit to a single label sequence of the same numbers although at one point it did get to 0.0086 (not tracked in wandb). Just feel like it should be trivially easy/stable to memorize one one-sec sequence idk\n* BUT good news: despite loss not being ~0, the output of the model is still the given sequence, so all is good heh\n* ^^ take it further and track wer metric in general AH or just the accuracy tbh like per token (if it doesn't match for example)\n\nbtw for original model/dataset (not ours! aka the audio one)\nshapes:\nlabels: torch.Size([1, 67]). encoder_outputs: torch.Size([1, 112, 768]). labels: torch.Size([1, 22]). encoder_outputs: torch.Size([1, 113, 768]).\n\nour model tho is like: encoder_outputs: torch.Size([1, 3137, 768]) labels: torch.Size([1, 16])  \nwith 2 batch size  \nlabel shape torch.Size([2, 20])  \nframe shape torch.Size([2, 33, 3, 224, 224])  \nencoder_outputs shape torch.Size([2, 3137, 768])  ","metadata":{}},{"cell_type":"code","source":"from transformers import SpeechT5ForSpeechToText, SpeechT5Config\nfrom datasets import load_metric\n\nds_dir = \"../input/data-32fps/data_32fps\"\n# frames_path = f\"{ds_dir}/processed/frames/somebody--twinke_twinkle_0\"\n# frames_path = f\"{ds_dir}/processed/frames/raw--twinke_twinkle_0\"\n# Our parameters\nTOKENIZER_PARAMS = {\n    \"pitch_range\": (21, 109),\n    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n    \"num_velocities\": 32,\n    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n    \"use_chords\": True,\n    \"use_rests\": False,\n    \"use_tempos\": True,\n    \"use_time_signatures\": False,\n    \"use_programs\": False,\n    \"num_tempos\": 32,  # number of tempo bins\n    \"tempo_range\": (40, 250),  # (min, max)\n}\ntok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n# wer_metric = load_metric(\"wer\")\n# Creates the tokenizer\ntokenizer = REMI(tok_config)\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\ndataset = Video2MIDIDataset(\n    root_dir=f\"{ds_dir}/processed\",\n    tokenizer=tokenizer,\n    image_processor=image_processor\n    # transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n)\n\n# TODO: add shuffle\ndataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn) # [  4, 189, 308,  34, 112, 140,  49, 111, 134, 199,  49, 112, 130]), # [  4, 189, 308,  32, 109, 140,  44, 110, 134, 199,  44, 111, 130])\nprint(f\"Training on subset: {dataset.piece_names}\")\n\nconfig = SpeechT5Config(\n    vocab_size=328,\n    d_model=768,\n    max_length=450\n)\n\nmodel = SpeechT5ForSpeechToText(config)\nmodel = model.to('cuda')  # Ensure your model is on GPU if available\nmodel_enc = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\nmodel_enc = model_enc.to('cuda')\nmodel.speecht5.encoder = None\n\nclass Config:\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 43\n    USE_WANDB = False\n    WANDB_PROJECT = \"video2music\"\n    WANDB_ENTITY = \"elles\"\n    WANDB_GROUP = \"full_twinkle\"\n    EXPERIMENT = WANDB_GROUP\n    \n    LOG_EVERY_X_EPOCHS = 1\n\n    LR = 5e-5 # 4.786300923226385e-05 # 0.0001\n    EPOCHS = 100\n    BATCH_SIZE = 5\n\n    # FPS = 32\n\ncfg = Config()\n\nif cfg.SEED:\n    set_seed(cfg.SEED)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR)\n# Training loop\nmodel.train()\n# STARZ\nif cfg.USE_WANDB:\n    # convert cfg class to dict\n    cfg_dict = cfg_to_dict(cfg)\n    cfg_dict.update(config.to_dict())\n    print(cfg_dict)\n    assert cfg_dict != {}, \"cfg_dict is empty\"\n    wandb_id = wandb.util.generate_id()\n    wandb.init(\n        project=cfg.WANDB_PROJECT,\n        name=cfg.EXPERIMENT,\n        entity=cfg.WANDB_ENTITY,\n        config=cfg_dict,\n        id=wandb_id,\n        resume=\"allow\",\n        group=cfg.WANDB_GROUP\n    )\n    wandb.watch(model, log=\"all\", log_freq=10)\n\ndataloader = DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=1, collate_fn=collate_fn)\nepochs = cfg.EPOCHS\n\nfor epoch in range(epochs):\n    total_correct = 0\n    total_tokens = 0\n    total_loss = 0\n    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}')\n    model.train()\n    for batch in progress_bar:\n  \n        optimizer.zero_grad()\n        frames = batch['frames'].to('cuda')\n        labels = batch['midi_tokens'].to('cuda')\n        with torch.no_grad():\n#             frames = batch['frames']\n\n            # labels = rand_labels.unsqueeze(0)\n#             labels = batch['midi_tokens']\n#             frames = frames.to('cuda')\n            # squeeze only if batch-size is > 1\n            frames = frames.squeeze(1)\n#             labels = labels.to('cuda')\n            outputs = model_enc(frames)\n            last_hidden_states = outputs.last_hidden_state\n#         new_encoder_outputs = last_hidden_states.to('cuda')\n        new_encoder_outputs = (last_hidden_states,)\n        inputs = {\n            \"encoder_outputs\": new_encoder_outputs,\n            \"labels\": labels # labels.unsqueeze(0) only unsqueeze if batch size is 1?\n        }\n        outputs = model(**inputs)\n\n        # Loss computation\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n        del frames, labels, outputs, last_hidden_states, new_encoder_outputs, inputs\n        torch.cuda.empty_cache()\n        progress_bar.set_postfix({'loss': loss.item()})\n\n    # Logging to wandb\n    average_loss = total_loss / len(dataloader)\n    if epoch % cfg.LOG_EVERY_X_EPOCHS == 0:\n        # TODO: add eval wer-metric\n        model.eval()\n        # TODO: iterate over training dataloader or eval dataloader\n        # because right now we're just using last batch output\n        with torch.no_grad():\n            inputs = {\n                \"encoder_outputs\": new_encoder_outputs,\n                \"labels\": labels #labels.unsqueeze(0) only unsqueeze if batch size is 1?\n            }\n            outputs = model(**inputs)\n            accuracy, batch_corr, batch_tot = compute_accuracy(outputs, labels)\n\n            # Accumulate results\n            # TODO fix: this is not accumulating per batch..\n            total_correct += batch_corr\n            total_tokens += batch_tot\n\n            overall_accuracy = total_correct / total_tokens\n            stats = {'epoch': epoch, 'epoch_avg_loss': average_loss, 'batch_accuracy': overall_accuracy}\n            print(stats)\n            if wandb.run:\n                wandb.log(stats)\n    del frames, labels, outputs, last_hidden_states, new_encoder_outputs, inputs\n    torch.cuda.empty_cache()\n\n# End of training\nif wandb.run:\n  wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T14:21:14.093973Z","iopub.execute_input":"2024-05-17T14:21:14.094365Z","iopub.status.idle":"2024-05-17T14:21:30.710896Z","shell.execute_reply.started":"2024-05-17T14:21:14.094329Z","shell.execute_reply":"2024-05-17T14:21:30.709193Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2049488167.py:21: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n  tok_config = TokenizerConfig(**TOKENIZER_PARAMS)\n","output_type":"stream"},{"name":"stdout","text":"Training on subset: ['raw--twinke_twinkle_0', 'raw--twinke_twinkle_1', 'raw--twinke_twinkle_11', 'raw--twinke_twinkle_16', 'raw--twinke_twinkle_24', 'raw--twinke_twinkle_17', 'raw--twinke_twinkle_28', 'raw--twinke_twinkle_6', 'raw--twinke_twinkle_5', 'raw--twinke_twinkle_10', 'raw--twinke_twinkle_4', 'raw--twinke_twinkle_26', 'raw--twinke_twinkle_7', 'raw--twinke_twinkle_19', 'raw--twinke_twinkle_8', 'raw--twinke_twinkle_3', 'raw--twinke_twinkle_2', 'raw--twinke_twinkle_27', 'raw--twinke_twinkle_22', 'raw--twinke_twinkle_12', 'raw--twinke_twinkle_20', 'raw--twinke_twinkle_13', 'raw--twinke_twinkle_9', 'raw--twinke_twinkle_18', 'raw--twinke_twinkle_23', 'raw--twinke_twinkle_15', 'raw--twinke_twinkle_21', 'raw--twinke_twinkle_25', 'raw--twinke_twinkle_14']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/100:   0%|          | 0/6 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  return torch.tensor(value)\nEpoch 1/100:   0%|          | 0/6 [00:12<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m             frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#             labels = labels.to('cuda')\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m             last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#         new_encoder_outputs = last_hidden_states.to('cuda')\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vivit/modeling_vivit.py:577\u001b[0m, in \u001b[0;36mVivitModel.forward\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    575\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m--> 577\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vivit/modeling_vivit.py:348\u001b[0m, in \u001b[0;36mVivitEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    341\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    342\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    343\u001b[0m         hidden_states,\n\u001b[1;32m    344\u001b[0m         layer_head_mask,\n\u001b[1;32m    345\u001b[0m         output_attentions,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 348\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vivit/modeling_vivit.py:291\u001b[0m, in \u001b[0;36mVivitLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 291\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# in Vivit, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vivit/modeling_vivit.py:235\u001b[0m, in \u001b[0;36mVivitAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    231\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    232\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 235\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    239\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vivit/modeling_vivit.py:160\u001b[0m, in \u001b[0;36mVivitSelfAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_layer, key_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 160\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.73 GiB is free. Process 4316 has 13.02 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.73 GiB is free. Process 4316 has 13.02 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  inputs = {\n        \"encoder_outputs\": last_hidden_states.to('cuda'),\n        \"decoder_input_ids\": labels.unsqueeze(0)\n        # \"attention_mask\": batch[\"attention_mask\"].to('cuda'),\n        # \"labels\": batch[\"decoder_input_ids\"].to('cuda'), # TODO: this for sure gets shifted automatically by the library no?\n  }\n  outputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:20:25.925933Z","iopub.execute_input":"2024-05-17T12:20:25.926879Z","iopub.status.idle":"2024-05-17T12:20:25.945260Z","shell.execute_reply.started":"2024-05-17T12:20:25.926839Z","shell.execute_reply":"2024-05-17T12:20:25.944177Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def compute_accuracy(outputs, labels):\n    logits = outputs.logits\n    prediction_ids = torch.argmax(logits, dim=-1)\n    # Flatten the tensors to compare each token\n    prediction_ids = prediction_ids.view(-1)\n    labels = labels.view(-1)\n    \n    # Compare predictions with labels\n    correct = (prediction_ids == labels).sum().item()\n    total = labels.size(0)\n\n    accuracy = correct / total\n    return accuracy, correct, total","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:51:05.460533Z","iopub.execute_input":"2024-05-17T12:51:05.460972Z","iopub.status.idle":"2024-05-17T12:51:05.467356Z","shell.execute_reply.started":"2024-05-17T12:51:05.460937Z","shell.execute_reply":"2024-05-17T12:51:05.466474Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Check that the trained model is outputting correct sequence","metadata":{}},{"cell_type":"code","source":"compute_accuracy(outputs, labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:28:17.024637Z","iopub.execute_input":"2024-05-17T12:28:17.025018Z","iopub.status.idle":"2024-05-17T12:28:17.032163Z","shell.execute_reply.started":"2024-05-17T12:28:17.024987Z","shell.execute_reply":"2024-05-17T12:28:17.031156Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}]},{"cell_type":"code","source":"  logits = outputs.logits\n  predicted_ids = torch.argmax(logits, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:21.314149Z","iopub.execute_input":"2024-05-17T12:21:21.315011Z","iopub.status.idle":"2024-05-17T12:21:21.338984Z","shell.execute_reply.started":"2024-05-17T12:21:21.314976Z","shell.execute_reply":"2024-05-17T12:21:21.337743Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"predicted_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:24.918999Z","iopub.execute_input":"2024-05-17T12:21:24.919626Z","iopub.status.idle":"2024-05-17T12:21:24.926423Z","shell.execute_reply.started":"2024-05-17T12:21:24.919588Z","shell.execute_reply":"2024-05-17T12:21:24.925467Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[  4, 189, 308,  32, 109, 140,  44, 110, 134, 199,  44, 111, 130]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2024-05-17T12:21:34.861775Z","iopub.execute_input":"2024-05-17T12:21:34.862698Z","iopub.status.idle":"2024-05-17T12:21:34.868828Z","shell.execute_reply.started":"2024-05-17T12:21:34.862659Z","shell.execute_reply":"2024-05-17T12:21:34.867915Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([  4, 189, 308,  32, 109, 140,  44, 110, 134, 199,  44, 111, 130],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"# tokens = midi_tokens[batch_i]\n# tokenss.append(tokens)\n# midi = tokenizer([tokens])\n# midis.append(midi)\n# midi_translation = [token_id_to_token[int(id_.detach().numpy())] for id_ in tokens]\n# midi_translations.append(midi_translation)","metadata":{},"execution_count":null,"outputs":[]}]}