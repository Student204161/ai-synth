{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXCIQdSVemQO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthesize data"
      ],
      "metadata": {
        "id": "cxmTK1r6ez8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment if not installed already:\n",
        "!pip install einops\n",
        "!!apt-get install timidity\n",
        "!pip install pretty_midi\n"
      ],
      "metadata": {
        "id": "Gyo3ULqofN4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042d3c51-3a7e-4be3-a307-1d735ac68000"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m741.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.25.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Collecting packaging~=23.1 (from mido>=1.1.16->pretty_midi)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592289 sha256=2e082708cd1597d38921bac9964fba9b2dec37d60f87c046bbc2c006f23782fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: packaging, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed mido-1.3.2 packaging-23.2 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Functions to synthesize data...\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import pretty_midi\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import torchaudio\n",
        "import librosa\n",
        "import shutil\n",
        "\n",
        "accidentals = \"flat\"\n",
        "\n",
        "white_notes = {0: \"C\", 2: \"D\", 4: \"E\", 5: \"F\", 7: \"G\", 9: \"A\", 11: \"B\"}\n",
        "sharp_notes = {1: \"C#\", 3: \"D#\", 6: \"F#\", 8: \"G#\", 10: \"A#\"}\n",
        "flat_notes  = {1: \"Bb\", 3: \"Eb\", 6: \"Gb\", 8: \"Ab\", 10: \"Bb\"}\n",
        "\n",
        "white_notes_scale = {0: 0, 2: 1, 4: 2, 5: 3, 7: 4, 9: 5, 11: 6}\n",
        "\n",
        "print_notes_to_stdout = False\n",
        "\n",
        "\n",
        "def note_breakdown(midi_note):\n",
        "    note_in_chromatic_scale = midi_note % 12\n",
        "    octave = round((midi_note - note_in_chromatic_scale) / 12 - 1)\n",
        "\n",
        "    return [note_in_chromatic_scale, octave]\n",
        "\n",
        "def is_white_key(note):\n",
        "    return (note % 12) in white_notes\n",
        "\n",
        "def pixel_range(midi_note, image_width):\n",
        "    # Returns the min and max x-values for a piano key, in pixels.\n",
        "\n",
        "    width_per_white_key = image_width / 52\n",
        "\n",
        "    if is_white_key(midi_note):\n",
        "        [in_scale, octave] = note_breakdown(midi_note)\n",
        "        offset = 0\n",
        "        width = 1\n",
        "    else:\n",
        "        [in_scale, octave] = note_breakdown(midi_note - 1)\n",
        "        offset = 0.5\n",
        "        width = 0.5\n",
        "\n",
        "    white_note_n = white_notes_scale[in_scale] + 7*octave - 5\n",
        "\n",
        "    start_pixel = round(width_per_white_key*(white_note_n + offset)) + 1\n",
        "    end_pixel    = round(width_per_white_key*(white_note_n + 1 + offset)) - 1\n",
        "\n",
        "    if width != 1:\n",
        "        mid_pixel = round(0.5*(start_pixel + end_pixel))\n",
        "        half_pixel_width = 0.5*width_per_white_key\n",
        "        half_pixel_width *= width\n",
        "\n",
        "        start_pixel = round(mid_pixel - half_pixel_width)\n",
        "        end_pixel    = round(mid_pixel + half_pixel_width)\n",
        "\n",
        "    return [start_pixel, end_pixel]\n",
        "\n",
        "def create_video(input_midi: str,\n",
        "        image_width = 360,\n",
        "        img_resize = 224, #resize to base2 number after using synthviz code\n",
        "        image_height = 360,\n",
        "        black_key_height = 2/3,\n",
        "        piano_height=64,\n",
        "        falling_note_color = [75, 105, 177],     # darker blue\n",
        "        pressed_key_color = [220, 10, 10], # lighter blue\n",
        "        vertical_speed = 1/4, # Speed in main-image-heights per second\n",
        "        fps = 16,\n",
        "        end_t = 0.0,\n",
        "        silence = 0.0,\n",
        "        sample_rate = 16000,\n",
        "        split=\"train\"\n",
        "    ):\n",
        "\n",
        "    midi_save_name = input_midi.split('/')[-1].split('.')[0]  #(str_list[0] + '~' + str_list[1]).split('.')[0]\n",
        "    frames_folder = os.path.join( f\"data/processed/{split}/frames\", midi_save_name)\n",
        "    piano_height = piano_height\n",
        "    main_height = image_height - piano_height\n",
        "    pixels_per_frame = main_height*vertical_speed / fps # (pix/image) * (images/s) / (frames / s) =\n",
        "\n",
        "\n",
        "    note_names = {}\n",
        "\n",
        "    for note in range(21, 109):\n",
        "        [note_in_chromatic_scale, octave] = note_breakdown(note)\n",
        "\n",
        "        if note_in_chromatic_scale in white_notes:\n",
        "            note_names[note] = \"{}{:d}\".format(\n",
        "                white_notes[note_in_chromatic_scale], octave)\n",
        "        else:\n",
        "            if accidentals == \"flat\":\n",
        "                note_names[note] = \"{}{:d}\".format(\n",
        "                    flat_notes[note_in_chromatic_scale], octave)\n",
        "            else:\n",
        "                note_names[note] = \"{}{:d}\".format(\n",
        "                    sharp_notes[note_in_chromatic_scale], octave)\n",
        "\n",
        "    # The 'notes' list will store each note played, with start and end\n",
        "    # times in seconds.\n",
        "    #print(\"Loading MIDI file:\", input_midi)\n",
        "    midi_data = pretty_midi.PrettyMIDI(input_midi)\n",
        "\n",
        "    try:\n",
        "        notes = [\n",
        "            { \"note\": n.pitch, \"start\": n.start, \"end\": n.end}\n",
        "            for n in midi_data.instruments[0].notes\n",
        "        ]\n",
        "    except:\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    #print(f\"Loaded {len(notes)} notes from MIDI file\")\n",
        "\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    # ~ The rest of the code is about making the video. ~\n",
        "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "    if not os.path.isdir(frames_folder):\n",
        "        os.makedirs(frames_folder)\n",
        "\n",
        "    # Delete all previous image frames:\n",
        "    for f in os.listdir(frames_folder):\n",
        "        os.remove(\"{}/{}\".format(frames_folder, f))\n",
        "\n",
        "    im_base = np.zeros((image_height, image_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # Draw the piano, and the grey lines next to the C's for the main area:\n",
        "    key_start = image_height - piano_height #0\n",
        "    white_key_end = image_height - 1\n",
        "    black_key_end = round(image_height - (1-black_key_height)*piano_height)\n",
        "\n",
        "    im_lines = im_base.copy()\n",
        "\n",
        "    for i in range(21, 109):\n",
        "        # draw white keys\n",
        "        if is_white_key(i):\n",
        "            [x0, x1] = pixel_range(i, image_width)\n",
        "            im_base[key_start:white_key_end, x0:x1] = [255, 255, 255]\n",
        "\n",
        "        # draw lines separating octaves\n",
        "        if i % 12 == 0:\n",
        "            im_lines[0:(key_start-1), (x0-2):(x0-1)] = [20, 20, 20]\n",
        "\n",
        "    for i in range(21, 109):\n",
        "        # draw black keys\n",
        "        if not is_white_key(i):\n",
        "            [x0, x1] = pixel_range(i, image_width)\n",
        "            im_base[key_start:black_key_end, x0:x1] = [0, 0, 0]\n",
        "\n",
        "    im_piano = im_base[key_start:white_key_end, :]\n",
        "\n",
        "    im_frame = im_base.copy()\n",
        "    im_frame += im_lines\n",
        "\n",
        "    # Timidity (the old version that I have!) always starts the audio\n",
        "    # at time = 0.  Add a second of silence to the start, and also\n",
        "    # keep making frames for a second at the end:\n",
        "    frame_start = 0 #min(note[\"start\"] for note in notes) - silence\n",
        "    end_t = max(note[\"end\"] for note in notes) if end_t == 0.0 else end_t  #### set as constant\n",
        "\n",
        "    #first frame init to start conditions\n",
        "    img = PIL.Image.fromarray(im_frame)\n",
        "    #img.save(\"{}/frame00000.png\".format(frames_folder))\n",
        "\n",
        "\n",
        "    # Rest of video:\n",
        "    finished = False\n",
        "    frame_ct = 0\n",
        "    pixel_start = 0\n",
        "    pixel_start_rounded = 0\n",
        "\n",
        "    #print(\"[Step 1/3] Generating video frames from notes\")\n",
        "\n",
        "\n",
        "    #pbar = tqdm.tqdm(total=end_t, desc='Creating video')\n",
        "    while not finished:\n",
        "        prev_pixel_start_rounded = pixel_start_rounded\n",
        "        pixel_start += pixels_per_frame\n",
        "        pixel_start_rounded = round(pixel_start)\n",
        "\n",
        "        pixel_increment = pixel_start_rounded - prev_pixel_start_rounded\n",
        "\n",
        "        #pbar.update(1/fps)\n",
        "        frame_start += 1/fps\n",
        "\n",
        "        #pbar.set_description(f'Creating video [Frame count = {frame_ct}]')\n",
        "\n",
        "        # # Copy most of the previous frame into the new frame:\n",
        "        im_frame[pixel_increment:main_height, :] = im_frame[0:(main_height - pixel_increment), :]\n",
        "        im_frame[0:pixel_increment, :] = im_lines[0:pixel_increment, :]\n",
        "        im_frame[key_start:white_key_end, :] = im_piano\n",
        "        # Which keys need to be colored?\n",
        "        # TODO(jxm): put notes in some data structure or something to make this much faster\n",
        "        keys_to_color = []\n",
        "        for note in notes:\n",
        "            if (note[\"start\"]) <= frame_start <= note[\"end\"]:\n",
        "                keys_to_color.append(note[\"note\"])\n",
        "\n",
        "        # First color the white keys (this will cover some black-key pixels),\n",
        "        # then re-draw the black keys either side,\n",
        "        # then color the black keys.\n",
        "        for note in keys_to_color:\n",
        "            if is_white_key(note):\n",
        "                [x0, x1] = pixel_range(note, image_width)\n",
        "                im_frame[key_start:white_key_end, x0:x1] = pressed_key_color\n",
        "\n",
        "        for note in keys_to_color:\n",
        "            if is_white_key(note):\n",
        "                if (not is_white_key(note - 1)) and (note > 21):\n",
        "                    [x0, x1] = pixel_range(note - 1, image_width)\n",
        "                    im_frame[key_start:black_key_end, x0:x1] = [0,0,0]\n",
        "\n",
        "                if (not is_white_key(note + 1)) and (note < 108):\n",
        "                    [x0, x1] = pixel_range(note + 1, image_width)\n",
        "                    im_frame[key_start:black_key_end, x0:x1] = [0,0,0]\n",
        "\n",
        "        for note in keys_to_color:\n",
        "            if not is_white_key(note):\n",
        "                [x0, x1] = pixel_range(note, image_width)\n",
        "                im_frame[key_start:black_key_end, x0:x1] = pressed_key_color\n",
        "\n",
        "\n",
        "        img = PIL.Image.fromarray(im_frame)\n",
        "\n",
        "        #img = img.resize((img_resize,img_resize))\n",
        "\n",
        "        frame_ct += 1\n",
        "        img.save(\"{}/frame{:05d}.png\".format(frames_folder, frame_ct))\n",
        "\n",
        "        if frame_start >= end_t:\n",
        "            finished = True\n",
        "\n",
        "\n",
        "    #pbar.close()\n",
        "\n",
        "    # print(\"[Step 2/3] Rendering MIDI to audio with Timidity\")\n",
        "    wav_path = f'data/processed/{split}/wavs/'+midi_save_name+'.wav'\n",
        "    save_wav_cmd = f\"timidity {input_midi} -OwM --preserve-silence -s {sample_rate} -A120 -o {wav_path} \"\n",
        "    save_wav_cmd = save_wav_cmd.split()\n",
        "    # save_wav_cmd[1], save_wav_cmd[-1] = input_midi, sound_file\n",
        "    subprocess.call(save_wav_cmd)\n",
        "\n",
        "    #if wav length is longer than end_t * sample rate, cut the end of the wav off\n",
        "    wav, sr = torchaudio.load(wav_path) # should be 25600 or anything base2.\n",
        "    target_len = int(sr*(end_t))\n",
        "    wav = pad_tensor(wav, (1,target_len))\n",
        "    #save_wav\n",
        "    torchaudio.save(wav_path, wav, sr)\n",
        "\n",
        "    # print(\"Skipped - [Step 3/3] Rendering full video with ffmpeg\")\n",
        "    # #Running from a terminal, the long filter_complex argument needs to\n",
        "    # #be in double-quotes, but the list form of subprocess.call requires\n",
        "    # #_not_ double-quoting.\n",
        "\n",
        "    spec = librosa.feature.melspectrogram(y=wav.numpy(),\n",
        "                                        sr=sr,\n",
        "                                            n_fft=4096,\n",
        "                                            hop_length=512,\n",
        "                                            win_length=None,\n",
        "                                            window='hann',\n",
        "                                            center=False,\n",
        "                                            pad_mode='constant',\n",
        "                                            power=2.0,\n",
        "                                            fmin=1,\n",
        "                                            fmax=5000, #pitch 109 / piano key 88 has frequency centered around 4200~ so just let fmax be 5000  https://inspiredacoustics.com/en/MIDI_note_numbers_and_center_frequencies\n",
        "                                            n_mels=88) #88 piano keys, means 88 bins\n",
        "    #print(spec.shape)\n",
        "    #assert spec.shape[1] == spec.shape[2]\n",
        "    spec = spec.squeeze()\n",
        "    #spec = torch.nn.functional.interpolate(torch.tensor(spec).unsqueeze(0),size=(80,128)).squeeze().permute(1,0)\n",
        "    #plt.imshow(spec)\n",
        "    #plt.savefig(f'data/processed/{split}/spectrograms/{midi_save_name}.png')\n",
        "    torch.save(spec,f'data/processed/{split}/spectrograms_pt/{midi_save_name}.pt')\n",
        "\n",
        "\n",
        "    mp4_path = os.path.join(f\"data/processed/{split}/videos\", midi_save_name)\n",
        "    ffmpeg_cmd = f\"ffmpeg -loglevel quiet -framerate {fps} -i {frames_folder}/frame%05d.png -i {wav_path} -f lavfi -t {end_t} -i anullsrc -filter_complex [1]adelay={0}|{0}[aud];[2][aud]amix -c:v libx264 -vf fps={fps} -pix_fmt yuv420p -y -strict -2 {mp4_path}.mp4 \"\n",
        "    #print(\"> ffmpeg_cmd: \", ffmpeg_cmd)\n",
        "    subprocess.call(ffmpeg_cmd.split())\n",
        "\n",
        "\n",
        "\n",
        "def pad_tensor(tensor, target_shape):\n",
        "    \"\"\"\n",
        "    Pad a tensor to a target shape with zeros if necessary.\n",
        "\n",
        "    Args:\n",
        "    - tensor: NumPy array, the input tensor\n",
        "    - target_shape: tuple, the target shape\n",
        "\n",
        "    Returns:\n",
        "    - padded_tensor: Tensor\n",
        "    \"\"\"\n",
        "    current_shape = tensor.shape\n",
        "    if current_shape[1] < target_shape[1]:\n",
        "        tensor = torch.nn.functional.pad(tensor, (0,target_shape[1] - current_shape[1]))\n",
        "    elif current_shape[1] > target_shape[1]:\n",
        "        tensor = tensor[:,:target_shape[1]]\n",
        "    return tensor\n",
        "\n",
        "\n",
        "\n",
        "# collect all frames into a torch.tensor .pt file of size (N,302,360,32,1)\n",
        "\n",
        "def save_to_pt(path_to_midi,split=\"train\"):\n",
        "\n",
        "    for root, dirs, files in os.walk(path_to_midi):\n",
        "        for file in tqdm.tqdm(files):\n",
        "            if file.endswith('.mid'):\n",
        "\n",
        "                frames_folder = f'data/processed/{split}/frames/'+ file.split('.')[0]\n",
        "\n",
        "                frames = []\n",
        "                for img_file in os.listdir(frames_folder):\n",
        "                    if img_file.endswith('.png'):\n",
        "                        img_path = os.path.join(frames_folder, img_file)\n",
        "                        #load img\n",
        "                        img = PIL.Image.open(img_path)\n",
        "                        #to numpy\n",
        "                        img = np.array(img)\n",
        "                        ## to grey scale\n",
        "                        ##img = img[:,:,0]\n",
        "                        #to tensor\n",
        "                        img_tensor = torch.from_numpy(img)\n",
        "                        frames.append(img_tensor)\n",
        "\n",
        "                frames = torch.stack(frames)\n",
        "                torch.save(frames, f'data/processed/{split}/frames_pt/'+file.split('.')[0]+'.pt')\n",
        "\n",
        "def make_synthetic(N,split,max_notes=3):\n",
        "\n",
        "    if not os.path.exists(f'data/processed/{split}/midi'):\n",
        "        os.makedirs(f'data/processed/{split}/midi')\n",
        "    if not os.path.exists(f'data/processed/{split}/videos'):\n",
        "        os.makedirs(f'data/processed/{split}/videos')\n",
        "    if not os.path.exists(f'data/processed/{split}/wavs'):\n",
        "        os.makedirs(f'data/processed/{split}/wavs')\n",
        "    if not os.path.exists(f'data/processed/{split}/spectrograms'):\n",
        "        os.makedirs(f'data/processed/{split}/spectrograms')\n",
        "    if not os.path.exists(f'data/processed/{split}/spectrograms_pt'):\n",
        "        os.makedirs(f'data/processed/{split}/spectrograms_pt')\n",
        "    if not os.path.exists(f'data/processed/{split}/frames_pt'):\n",
        "        os.makedirs(f'data/processed/{split}/frames_pt')\n",
        "\n",
        "    note_count = np.random.randint(1,max_notes + 1, N)\n",
        "    tot_notes = np.sum(note_count)\n",
        "    #midi piano starts at 21 to 109.\n",
        "    lowest_pitch = 21\n",
        "    highest_pitch = 109\n",
        "    pitches  = (torch.randperm(tot_notes) % (highest_pitch-lowest_pitch)).numpy() + lowest_pitch\n",
        "    #velocitys  = (torch.randperm(tot_notes) % 20).numpy()\n",
        "\n",
        "    note_index = 0\n",
        "    for i in range(N):\n",
        "        midi = pretty_midi.PrettyMIDI()\n",
        "        instrument = pretty_midi.Instrument(0)\n",
        "\n",
        "        for note in range(note_count[i]):\n",
        "            pitch = pitches[note_index]\n",
        "            velocity = 100\n",
        "            #clip starts at 0 and ends at sec 1\n",
        "            min_duration = 0.15\n",
        "            start = np.random.uniform(0.1,0.7*2)\n",
        "            end = np.random.uniform(start+min_duration, 2 - 0.1)\n",
        "            note = pretty_midi.Note(velocity=velocity, pitch=pitch, start=start, end=end)\n",
        "            instrument.notes.append(note)\n",
        "            note_index += 1\n",
        "        midi.instruments.append(instrument)\n",
        "        #save midi\n",
        "        string_i = '{:0>5}'.format(str(i))\n",
        "        midi.write(f'data/processed/{split}/midi/syn_{string_i}.mid')\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import threading\n",
        "\n",
        "def synth_create_vids_worker(split, midi_files, image_width, image_height, piano_height, fps, segm_length, sample_rate, pbar):\n",
        "    for midi_file in midi_files:\n",
        "        if not os.path.exists(os.path.join(f'data/processed/{split}/spectrograms_pt', midi_file.replace('.mid','.pt'))):\n",
        "            create_video(\n",
        "                input_midi=os.path.join(f'data/processed/{split}/midi', midi_file),\n",
        "                image_width=image_width,\n",
        "                image_height=image_height,\n",
        "                piano_height=piano_height,\n",
        "                fps=fps,\n",
        "                end_t=segm_length,\n",
        "                sample_rate=sample_rate,\n",
        "                split=split\n",
        "            )\n",
        "            pbar.update(1)\n",
        "\n",
        "def synth_create_vids(split, image_width=512, image_height=16, piano_height=16, segm_length=2, sample_rate=16000, fps=16, num_threads=8):\n",
        "    # Find paths to all midi files\n",
        "    midi_files = [file for file in os.listdir(f'data/processed/{split}/midi') if file.endswith('.mid')]\n",
        "    midi_files.sort()\n",
        "\n",
        "    # Split midi_files into chunks for each thread\n",
        "    chunks = [midi_files[i:i + len(midi_files)//num_threads] for i in range(0, len(midi_files), len(midi_files)//num_threads)]\n",
        "\n",
        "    # Initialize progress bar\n",
        "    total_files = sum(len(chunk) for chunk in chunks)\n",
        "    with tqdm.tqdm(total=total_files) as pbar:\n",
        "        # Create threads\n",
        "        threads = []\n",
        "        for chunk in chunks:\n",
        "            t = threading.Thread(target=synth_create_vids_worker, args=(split, chunk, image_width, image_height, piano_height, fps, segm_length, sample_rate, pbar))\n",
        "            threads.append(t)\n",
        "\n",
        "        # Start threads\n",
        "        for thread in threads:\n",
        "            thread.start()\n",
        "\n",
        "        # Wait for all threads to finish\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "\n"
      ],
      "metadata": {
        "id": "WwLcCXp1e2bl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    #torch set seed\n",
        "    torch.manual_seed(0)\n",
        "    import random\n",
        "    random.seed(0)\n",
        "    import numpy as np\n",
        "    np.random.seed(0)\n",
        "\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk('data/raw'):\n",
        "        for file in files:\n",
        "            if file.endswith('.mid'):\n",
        "                all_files.append(os.path.join(root, file))\n",
        "    all_files =np.asarray(all_files)\n",
        "    N_tot = len(all_files)\n",
        "\n",
        "    segm_length = 2 #in sec\n",
        "    N_train,N_val,N_test = 200, 100,100\n",
        "    split = \"train\"\n",
        "    make_synthetic(N_train,split)\n",
        "    synth_create_vids(split,segm_length=segm_length) #using defaultvalues / 32x3x16x512 ...\n",
        "    save_to_pt(f'data/processed/{split}/midi', split=split)\n",
        "\n",
        "    split = \"val\"\n",
        "    make_synthetic(N_val,split)\n",
        "    synth_create_vids(split)\n",
        "    save_to_pt(f'data/processed/{split}/midi', split=split)"
      ],
      "metadata": {
        "id": "6js-iqxQenqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataLoader"
      ],
      "metadata": {
        "id": "YQjt1khYfv-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "from torchvision import transforms\n",
        "#import wandb\n",
        "import sys,os\n",
        "import soundfile\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from transformers import SpeechT5FeatureExtractor\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        # self.max_waveform_length = max_waveform_length\n",
        "        # self.max_num_frames = max_num_frames #\n",
        "        self.frames = [file for file in os.listdir(os.path.join(root_dir, 'frames_pt')) if (file.endswith('.pt') )]\n",
        "        self.frames.sort()\n",
        "        self.transf = transforms.Compose([\n",
        "            transforms.Resize((88, 55)),  # Resize the image if needed\n",
        "        ])\n",
        "        #self.feature_extractor = feature_extractor\n",
        "        # checkpoint = \"microsoft/speecht5_tts\"\n",
        "        # self.feature_extractor = SpeechT5FeatureExtractor(fmin=6,fmax=10000,do_normalize=True,num_mel_bins=128)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_file = os.path.join(self.root_dir, 'wavs', self.frames[idx].replace('.pt', '.wav'))\n",
        "        frames_path = os.path.join(self.root_dir, 'frames_pt', self.frames[idx])\n",
        "        spectrogram_path = os.path.join(self.root_dir, 'spectrograms_pt', self.frames[idx])\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(wav_file)\n",
        "\n",
        "        frames = torch.load(frames_path).unsqueeze(0)\n",
        "        frames = frames.float() / 255 #\n",
        "\n",
        "        frames = frames.permute(0, 1, 4, 2, 3)\n",
        "        spectrogram = torch.tensor(torch.load(spectrogram_path)).unsqueeze(0)\n",
        "        spectrogram = (spectrogram/torch.max(spectrogram)) #normalize\n",
        "\n",
        "        name = self.frames[idx].replace('.pt', '')\n",
        "\n",
        "\n",
        "        return frames, spectrogram, name\n",
        "def collate_fn(batch):\n",
        "    frames, spectrogram, name = zip(*batch)\n",
        "    dat = {'frames':torch.vstack(frames).to(device=device), 'spectrogram':  torch.vstack(spectrogram).to(device=device), 'name':name}\n",
        "    return dat\n",
        "\n",
        "\n",
        "#init model\n",
        "batch_size=8\n",
        "train_dataset = CustomDataset(root_dir='data/processed/train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataset = CustomDataset(root_dir='data/processed/val')\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "print('Train data N:',len(train_dataset))\n",
        "print('Val data N:',len(val_dataset))\n",
        "day = iter(train_dataset)\n",
        "fram, spe, name, spe_t5 = next(day)\n",
        "\n",
        "print(fram.shape)\n",
        "print(spe.shape)\n",
        "print(spe_t5.shape)\n",
        "print(name)\n"
      ],
      "metadata": {
        "id": "DcFzS8psfyfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "WcaxXPcLgm7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "\n",
        "from transformers.models.vivit.modeling_vivit import VivitModel, VivitConfig, VivitLayer, VivitEncoder\n",
        "from transformers.models.speecht5.modeling_speecht5 import SpeechT5Decoder, SpeechT5Config, SpeechT5SpeechDecoderPostnet\n",
        "\n",
        "class VivitTubeletEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct Vivit Tubelet embeddings.\n",
        "\n",
        "    This module turns a batch of videos of shape (batch_size, num_frames, num_channels, height, width) into a tensor of\n",
        "    shape (batch_size, seq_len, hidden_size) to be consumed by a Transformer encoder.\n",
        "\n",
        "    The seq_len (the number of patches) equals (number of frames // tubelet_size[0]) * (height // tubelet_size[1]) *\n",
        "    (width // tubelet_size[2]).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_frames = config.num_frames\n",
        "        self.image_size = config.image_size\n",
        "        self.patch_size = config.tubelet_size\n",
        "        # print(self.image_size)\n",
        "        # print(self.patch_size)\n",
        "        self.num_patches = (\n",
        "            (self.image_size[1] // self.patch_size[2]) # 256/16\n",
        "            * (self.image_size[0] // self.patch_size[1]) # 16/4\n",
        "            * (self.num_frames // self.patch_size[0]) # 32/2\n",
        "        )\n",
        "        self.embed_dim = config.hidden_size\n",
        "\n",
        "        self.projection = nn.Conv3d(\n",
        "            config.num_channels, config.hidden_size, kernel_size=config.tubelet_size, stride=config.tubelet_size\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height},{width}) doesn't match model ({self.image_size},{self.image_size}).\"\n",
        "            )\n",
        "\n",
        "        # permute to (batch_size, num_channels, num_frames, height, width)\n",
        "        pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        x = self.projection(pixel_values)\n",
        "        # out_batch_size, out_num_channels, out_num_frames, out_height, out_width = x.shape\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VivitEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Vivit Embeddings.\n",
        "\n",
        "    Creates embeddings from a video using VivitTubeletEmbeddings, adds CLS token and positional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = VivitTubeletEmbeddings(config)\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(\n",
        "            torch.zeros(1, self.patch_embeddings.num_patches, config.hidden_size) #used to be +1 patch for cls tokens\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "\n",
        "        #cls_tokens = self.cls_token.tile([batch_size, 1, 1])\n",
        "        #embeddings = torch.cat((cls_tokens, embeddings), dim=1) #we dont do classificaiton, we do prediction so no cls tokens\n",
        "\n",
        "        # add positional encoding to each token\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class VivitPooler(nn.Module): #dont simply pool like this?\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, embeddings, encoder, decoder,decoder_postnet):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.decoder_postnet = decoder_postnet\n",
        "\n",
        "        #self.pool = torch.nn.MaxPool2d((2,1))\n",
        "        self.pool1 = torch.nn.MaxPool2d((1,4))\n",
        "        #self.mlp = torch.nn.Linear(128* 88, 88 * 55)\n",
        "        self.mlp1 = torch.nn.Linear(256 * 64, 88 * 55)\n",
        "\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, F, C, H, W = x.shape\n",
        "        x = self.embeddings(x)\n",
        "        #from second last dimension, drop the class tokens bcs we aren't doing classification\n",
        "        x = self.encoder(x)#.last_hidden_state#[:,1:]\n",
        "        #x = self.decoder(x.last_hidden_state)\n",
        "        x = self.pool1(x.last_hidden_state).flatten(1)\n",
        "        # x , _ , _ = self.decoder_postnet(x.last_hidden_state)\n",
        "\n",
        "        #print(x.shape)\n",
        "        # x = self.pool(x).flatten(1)\n",
        "        x = self.mlp1(x).reshape(B, 88, 55)\n",
        "        #x = self.activation(x)\n",
        "        return x #torch.clip(x,max=10)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    img = torch.ones([2, 32, 3, 16, 512]).cuda()\n",
        "\n",
        "    vivit_dict = {\n",
        "        \"image_size\": (16,512),\n",
        "        \"num_frames\": 32,\n",
        "        \"tubelet_size\": [4, 16, 32],\n",
        "        \"num_channels\": 3,\n",
        "        \"hidden_size\": 512,\n",
        "        \"num_hidden_layers\": 4,\n",
        "        \"num_attention_heads\": 256,\n",
        "        \"intermediate_size\": 512,\n",
        "        \"hidden_act\": 'gelu_fast',\n",
        "        \"hidden_dropout_prob\": 0.0,\n",
        "        \"attention_probs_dropout_prob\": 0.0,\n",
        "        \"initializer_range\": 0.02,\n",
        "        \"layer_norm_eps\": 1e-06,\n",
        "        \"qkv_bias\": True\n",
        "    }\n",
        "    config_vivit = VivitConfig(**vivit_dict)\n",
        "\n",
        "    conf_dict = {\n",
        "      \"num_mel_bins\": 88,\n",
        "      \"activation_dropout\": 0.1,\n",
        "      \"attention_dropout\": 0.1,\n",
        "      \"decoder_attention_heads\": 32,\n",
        "      \"decoder_ffn_dim\": 512,\n",
        "      \"decoder_layerdrop\": 0.1,\n",
        "      \"decoder_layers\": 2,\n",
        "      \"decoder_start_token_id\": 2,\n",
        "      \"hidden_act\": \"gelu\",\n",
        "      \"hidden_dropout\": 0.1,\n",
        "      \"hidden_size\": 512,\n",
        "      \"is_encoder_decoder\": True,\n",
        "      \"layer_norm_eps\": 1e-05,\n",
        "      \"mask_feature_length\": 4,\n",
        "      \"mask_feature_min_masks\": 0,\n",
        "      \"mask_feature_prob\": 0.0,\n",
        "      \"mask_time_length\": 4,\n",
        "      \"mask_time_min_masks\": 2,\n",
        "      \"positional_dropout\": 0.1,\n",
        "      \"transformers_version\": \"4.40.1\",\n",
        "      \"use_guided_attention_loss\": True,\n",
        "    }\n",
        "    embeddings = VivitEmbeddings(config_vivit)\n",
        "    encoder = VivitEncoder(config_vivit)\n",
        "    config_speecht5 = SpeechT5Config(**conf_dict)\n",
        "\n",
        "    decoder = SpeechT5Decoder(config_speecht5)\n",
        "    decoder_postnet = SpeechT5SpeechDecoderPostnet(config_speecht5)\n",
        "\n",
        "    model = SimpleModel(embeddings, encoder, decoder,decoder_postnet).cuda()\n",
        "\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "    print('Trainable Parameters: %.3fM' % parameters)\n",
        "\n",
        "    out = model(img)\n",
        "\n",
        "    print(\"Shape of out :\", out.shape)      # [B, num_classes]\n",
        "    print(\"dtype of out :\", out.dtype)      # float32\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_tjDoTpfydr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "5M8RBlDMgt5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "#wandb.login(key='<yourapikeyhere>') # or disable wandb if not configured.\n",
        "\n",
        "\n",
        "\n",
        "#opt\n",
        "optimizer = torch.optim.AdamW(model.parameters(),weight_decay=1e-6, lr = 1e-4)\n",
        "#     [\n",
        "#         {\"params\": model.embeddings.parameters(), \"lr\": 1e-3},\n",
        "#         {\"params\": model.encoder.parameters(), \"lr\": 1e-3},\n",
        "#         {\"params\": model.decoder.parameters(), \"lr\": 1e-3},\n",
        "#         {\"params\": model.decoder_postnet.parameters(), \"lr\": 1e-3},\n",
        "#         {\"params\": model.last_layer1.parameters(), \"lr\": 5e-2},\n",
        "#     ], weight_decay=1e-3\n",
        "# )\n",
        "\n",
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "print('Trainable Parameters: %.3fM' % parameters)\n",
        "\n",
        "#loss fun\n",
        "criterion = nn.L1Loss() #nn.L1Loss() #nn.CrossEntropyLoss() # no reason to use crossentropy if we dont work with classes...\n",
        "\n",
        "wandb.init(\n",
        "    project=\"test_aisynth_v2\",\n",
        "    name=\"vid2audio\",\n",
        "    job_type=\"training\",\n",
        "    reinit=True)\n",
        "\n",
        "# %% Fit the model\n",
        "# Number of epochs\n",
        "epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "step = 0\n",
        "val_interval=100\n",
        "save_interval=1000\n",
        "lr_update=1000\n",
        "plot_interval = 100\n",
        "#use tqdm to print train loss and val loss as updating instead of constantly printing\n",
        "\n",
        "val_loss = 10000\n",
        "for epoch in range(epochs):\n",
        "    train_epoch_loss =[]\n",
        "    val_epoch_loss =[]\n",
        "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs} - Train\") as pbar:\n",
        "        for dat in train_loader:\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(dat['frames'])\n",
        "            loss = criterion(out, dat['spectrogram'])\n",
        "            wandb.log({\"train_loss\": loss.detach().cpu().item()}, step=step)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_epoch_loss.append(loss.detach().cpu())\n",
        "            pbar.set_postfix({'train_loss': f'{loss:.4f}','val_loss': f'{val_loss:.4f}'})\n",
        "            pbar.update()\n",
        "\n",
        "            if step % plot_interval == 0:\n",
        "              fig, axes = plt.subplots(1, 2)\n",
        "              h = out.shape[2]\n",
        "              w = out.shape[1]\n",
        "\n",
        "              viz_out = out[0].detach().clone().cpu().numpy()\n",
        "              viz_ref = dat['spectrogram'][0].detach().clone().cpu().numpy()\n",
        "\n",
        "              # Plot the first image on the first subplot\n",
        "              axes[0].imshow(viz_out,aspect='auto', extent=(0, h, 0, w))\n",
        "              axes[0].set_title('predicted')\n",
        "\n",
        "              # Plot the second image on the second subplot\n",
        "              axes[1].imshow(viz_ref,aspect='auto', extent=(0, h, 0, w))\n",
        "              axes[1].set_title('GT')\n",
        "\n",
        "              plt.show()\n",
        "\n",
        "              # fig, axes = plt.subplots(1, 2)\n",
        "              # h = out.shape[2]\n",
        "              # w = out.shape[1]\n",
        "\n",
        "\n",
        "              # viz_out = np.exp(out[0].detach().clone().cpu().numpy())\n",
        "              # viz_ref = np.exp(dat['spectrogram'][0].detach().clone().cpu().numpy())\n",
        "\n",
        "              # # Plot the first image on the first subplot\n",
        "              # axes[0].imshow(viz_out,aspect='auto', extent=(0, h, 0, w))\n",
        "              # axes[0].set_title('predicted_log')\n",
        "\n",
        "              # # Plot the second image on the second subplot\n",
        "              # axes[1].imshow(viz_ref,aspect='auto', extent=(0, h, 0, w))\n",
        "              # axes[1].set_title('GT_log')\n",
        "\n",
        "              plt.show()\n",
        "\n",
        "            if step % val_interval == 0:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "\n",
        "                    val_dat = next(iter(val_loader))\n",
        "\n",
        "                    val_out = model(val_dat['frames'])\n",
        "\n",
        "                    val_loss = criterion(val_out, val_dat['spectrogram'])\n",
        "\n",
        "                    val_epoch_loss.append(val_loss.detach().cpu())\n",
        "\n",
        "\n",
        "                    fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "                    viz_out_val = val_out[0].detach().clone().cpu().numpy()\n",
        "                    viz_ref_val = val_dat['spectrogram'][0].detach().clone().cpu().numpy()\n",
        "\n",
        "                    # Plot the first image on the first subplot\n",
        "                    axes[0].imshow(viz_out_val,aspect='auto', extent=(0, h, 0, w))\n",
        "                    axes[0].set_title('validation predicted')\n",
        "\n",
        "                    # Plot the second image on the second subplot\n",
        "                    axes[1].imshow(viz_ref_val,aspect='auto', extent=(0, h, 0,w))\n",
        "                    axes[1].set_title('validation GT')\n",
        "\n",
        "                    plt.show()\n",
        "\n",
        "                    wandb.log({\"val_loss\": val_loss.detach().cpu().item()}, step=step)\n",
        "\n",
        "\n",
        "            step += 1\n",
        "            if step % save_interval == 0:\n",
        "              torch.save(model.state_dict(),f'ai_synth_model_works_{step}.pth')\n",
        "\n",
        "            if step % lr_update == 0:\n",
        "              scheduler.step()\n",
        "\n",
        "    # print(f'epoch train mean loss:{np.mean(train_epoch_loss)}')\n",
        "    # print(f'epoch val mean loss:{np.mean(val_epoch_loss)}')\n",
        "    wandb.log({\"avg_train_loss\": np.mean(train_epoch_loss)})\n",
        "    wandb.log({\"avg_val_loss\": np.mean(val_epoch_loss)})\n",
        "\n",
        "#1epoch train mean loss:6.293105602264404\n",
        "#1epoch val mean loss:8.45799922943115\n",
        "\n"
      ],
      "metadata": {
        "id": "X81kbZ1vgsgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAq5jmLei9gL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}